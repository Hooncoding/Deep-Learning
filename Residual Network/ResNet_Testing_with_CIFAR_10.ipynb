{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_Testing_with_CIFAR-10",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OANaZjxisFr"
      },
      "source": [
        "## Residual Network Practice with Pytorch\n",
        "\n",
        "이 note에서는 ResNet 논문을 기반으로 ResNet의 성능을 Testing 해본다.\n",
        "\n",
        "\n",
        "### 사용할 학습 데이터\n",
        "- ImageNet\n",
        "- CIFAR-10\n",
        "\n",
        "### 사용할 모델\n",
        "- ImageNet dataset\n",
        "  - Pre-trained Model\n",
        "- CIFAR-10\n",
        "  - Plain Net - 20, 56\n",
        "  - ResNet - 20, 56"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPkvv9vTm7nL"
      },
      "source": [
        "# Library Import\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxWRzJN8pEKy"
      },
      "source": [
        "# GPU setting\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRNSZsGvpP64"
      },
      "source": [
        "ImageNet 클래스(1000개) 정보 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzOrTWhVpW_5"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import json\n",
        "\n",
        "# ImageNet dataset class값 가져오기\n",
        "imagenet_json, _ = urlretrieve('https://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
        "with open(imagenet_json) as f:\n",
        "  imagenet_labels = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX1roEgepuuf",
        "outputId": "2ebf00ad-3cdb-4fd7-fcb5-7f8e55d141d6"
      },
      "source": [
        "# testing code\n",
        "print(imagenet_labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tench, Tinca tinca\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv5BBxfIqN04"
      },
      "source": [
        "Data 전처리는 논문과 같이 처리한다.  \n",
        "  \n",
        "\"The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation [41].\n",
        "A 224×224 crop is randomly sampled from an image or its\n",
        "horizontal flip\"(page 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWjOVdukqzgR"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "                                 transforms.Resize(256),\n",
        "                                 transforms.CenterCrop(224),\n",
        "                                 transforms.ToTensor() #PIL 혹은 np.array를 tensor로 바꾸어준다.\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBZbexDxreZ3"
      },
      "source": [
        "# image plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVhyuU6CrlqG"
      },
      "source": [
        "# 경로로 부터 이미지를 받아와 preprocess 단계를 거쳐 tensor로 변환\n",
        "def image_loader(path):\n",
        "  image = PIL.Image.open(path)\n",
        "  # 전처리를 거친 후 network 입력단에 들어갈 배치 목적의 차원 추가 (가로, 세로, 채널) => (배치, 가로, 세로, 채널)\n",
        "  image = preprocess(image).unsqueeze(0)\n",
        "  # 처리된 image를 device에 등록\n",
        "  return image.to(device, torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32fFy_SksGPB"
      },
      "source": [
        "# Preprocessing testing\n",
        "\n",
        "image_path = './drive/MyDrive/Dev-dl/cat.jpg'\n",
        "image = image_loader(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ35mkVDs_WW"
      },
      "source": [
        "#Test Image Plotting\n",
        "\n",
        "def imshow(tensor):\n",
        "  #matplotlib는 GPU를 지원하지 않으므로 CPU로 이동\n",
        "  image = tensor.cpu().clone()\n",
        "  #맨 앞 차원(배치) 제거\n",
        "  image = image.squeeze(0)\n",
        "  # PIL 객체로 변경\n",
        "  image = transforms.ToPILImage()(image)\n",
        "  plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "F4VIk2UNtcx2",
        "outputId": "8d01db54-d89c-4215-e16f-322164b462fb"
      },
      "source": [
        "plt.figure()\n",
        "imshow(image)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmZI31D0tiHm"
      },
      "source": [
        "###Pretrained 모델 불러오기\n",
        "\n",
        "- ImageNet dataset은 데이터 크기가 매우 크기 때문에 새로 학습을 시키는 것은 시간 상 생략\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z71LX34t9P2"
      },
      "source": [
        "# input data normalization class\n",
        "class Normalize(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalize, self).__init__()\n",
        "    #Normalize method => Standardization\n",
        "    self.register_buffer('mean', torch.Tensor(mean))\n",
        "    self.register_buffer('std', torch.Tensor(std))\n",
        "\n",
        "  def forward(self, input):\n",
        "    mean = self.mean.reshape(1, 3, 1, 1)\n",
        "    std = self.std.reshape(1, 3, 1, 1)\n",
        "    return (input - mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxwUoDonvcNW",
        "outputId": "0d8581e8-15b3-428c-81c6-eed66c90d396"
      },
      "source": [
        "# 모델 실행시 input에 대한 Normalize 수행 후 실행\n",
        "model = nn.Sequential(\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
        ").to(device).eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "993ihnsdv99W",
        "outputId": "ca103584-6b34-48e8-860d-385afc7a7747"
      },
      "source": [
        "# Test image를 넣어 Prediction 수행\n",
        "outputs = model(image)\n",
        "percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
        "# 가장 높은 값을 가지는 5개의 인덱스와 그 확률을 출력\n",
        "print(\"결과\")\n",
        "for i in outputs[0].topk(5)[1]:\n",
        "  print(f'인덱스: {i.item()}, 클래스명: {imagenet_labels[i]}, 확률: {round(percentages[i].item(),4)}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "결과\n",
            "인덱스: 281, 클래스명: tabby, tabby cat, 확률: 55.0171%\n",
            "인덱스: 285, 클래스명: Egyptian cat, 확률: 33.24%\n",
            "인덱스: 282, 클래스명: tiger cat, 확률: 11.1233%\n",
            "인덱스: 728, 클래스명: plastic bag, 확률: 0.0874%\n",
            "인덱스: 287, 클래스명: lynx, catamount, 확률: 0.0659%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSFm9pmWwzZG"
      },
      "source": [
        "## 다음으로 CIFAR-10 DATASET을 대상으로 실제 RESNET 모델을 Build 해보고 학습해본다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPWrtl9MzuZW"
      },
      "source": [
        "# Library import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TkegeZTz6Kc"
      },
      "source": [
        "# Residual Block class\n",
        "class ResidualBlock(nn.Module):\n",
        "  #in_plane : input의 dimension, planes: output의 dimension)\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    #ResidualBlock 호출 시 nn.Module 호출\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    # Convolution layer 정의\n",
        "    # Residual block은 두 개의 3x3 Conv layer로 구성되어 있으며 Conv => BN => Activation 과정을 거친다\n",
        "    # 논문에 따라 bias는 False로 지정\n",
        "    \n",
        "    # 첫 번째 Convolution Layer in Residual block\n",
        "    # filter 수가 2배씩 증가하므로 너비 x 높이를 2배로 줄이고자 할때는 pooling이 아닌 stride를 2로 조정\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    # Conv layer를 거쳐 나온 output을 Batch Normalize\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    # 두 번째 Convolution Layer in Residual block\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    \n",
        "    # Stride = 1일 경우 input dimension == output dimension => identity mapping 수행\n",
        "    # argument가 없이 nn.Sequential()을 호출할 경우 identity mapping 수행\n",
        "    self.shortcut = nn.Sequential()\n",
        "    # Stride = 2일 경우 input dimension != output dimension => stride=2, 1x1 합성곱망을 거치도록 함\n",
        "    if stride != 1:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          #여기서 planes가 아닌 in_planes가 들어가는 이유\n",
        "            #shortcut은 input에 대해 identity mapping을 하는 것이므로 input의 dimension을 받는다\n",
        "          nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes)\n",
        "      )\n",
        "  #순전파 순서 정의\n",
        "  def forward(self, x):\n",
        "    # Conv => BN => Activation 순으로 진행\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    #shortcut connection\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v7m_4YONwVr"
      },
      "source": [
        "### 레이어 구성 목표\n",
        "\n",
        "1. The first layer is 3×3 convolutions.\n",
        "2. Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively,\n",
        "with 2n layers for each feature map size. The numbers of\n",
        "filters are {16, 32, 64} respectively.\n",
        "3. The network ends\n",
        "with a global average pooling, a 10-way fully-connected\n",
        "layer, and softmax. \n",
        "4. Plain Net - 20, 56 / ResNet - 20, 56"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZBtarzQ6MK1"
      },
      "source": [
        "# ResNet 전체 Network 구성하는 Class\n",
        "class ResNet(nn.Module):\n",
        "  #CIFAR-10 데이터셋의 클래스 10개에 맞추어 Parameter 조정\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 16\n",
        "\n",
        "    # ImageNet 처리 시 가장 앞 단에 layer 7x7, maxPooling 층을 두었으나\n",
        "    # 3의 input dimension(RGB)를 받아 64개 feature map 생성\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "    self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    #num_blocks의 갯수 만큼 strides 리스트에 넣는다 => list의 length만큼 layer 내에 block 만든다\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes # 다음 layer로 넘어갈때 채널 수 맞춰주기\n",
        "    # *args: 가변 갯수의 인자를 함수에 집어넣어 줌\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  #순전파 방식\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out) #out: [batch_size, 64, 8,8]\n",
        "    #1x1로 바꿔주기 위해서 8x8 maxpolling\n",
        "    out = F.avg_pool2d(out, 8)\n",
        "    # view: pytorch에서 reshape과 같은 역할을 함\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjsulAB8VyOk"
      },
      "source": [
        "# ResNet20 함수 정의\n",
        "def ResNet20():\n",
        "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
        "  return ResNet(ResidualBlock, [3,3,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly81STdHWPj6"
      },
      "source": [
        "### CIFAR-10 DATASET LOADING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKnp-RRFaaQc"
      },
      "source": [
        "1. for training: 4 pixels are padded on each side,\n",
        "and a 32×32 crop is randomly sampled from the padded\n",
        "image or its horizontal flip\n",
        "2. For testing, we only evaluate\n",
        "the single view of the original 32×32 image\n",
        "\n",
        "3.  These models are trained with a minibatch size of 128 on two GPUs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuuY08eNYaax",
        "outputId": "af3f1edd-97a3-4b43-98fd-47a3de2e0478"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "                                      # 1번 조건\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                     # 2번 조건: test dataset에 대해서는 augument 진행 X\n",
        "                                     transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./drive/MyDrive/Dev-dl/data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./drive/MyDrive/Dev-dl/data/', train=False, download=True, transform=transform_test)\n",
        "# Overfitting을 피하기 위해 shuffle=True로 지정해준다. (매 epoch마다 데이터셋 섞음)\n",
        "# two GPU사용\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6XtAc2ebGUp"
      },
      "source": [
        "### 환경설정 및 학습 함수 정의\n",
        "\n",
        "1. We use a weight decay of 0.0001 and momentum of 0.9,\n",
        "and adopt the weight initialization in [13] and BN [16] but\n",
        "with no dropout.\n",
        "\n",
        "2. We start with a learning\n",
        "rate of 0.1, divide it by 10 at 32k and 48k iterations, and\n",
        "terminate training at 64k iterations, which is determined on\n",
        "a 45k/5k train/val split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcPJ5Hb6bYIA",
        "collapsed": true,
        "outputId": "6e954651-ee51-497f-b754-a28d1bd08279"
      },
      "source": [
        "device ='cuda'\n",
        "\n",
        "#신경망 선언\n",
        "net = ResNet20()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet20_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/ResNet20-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    # #100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/ResNet20-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "  torch.save(state, './checkpoint' + file_name)\n",
        "  print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1]\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "[2, 1, 1]\n",
            "16\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "[2, 1, 1]\n",
            "32\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcbK4Q2Sh52k"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWemAXcWlEWz",
        "collapsed": true,
        "outputId": "8ea508cc-aa07-4722-f3b4-03fe8a91bfbd"
      },
      "source": [
        "import time\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
        "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
        "  if epoch >=90:\n",
        "    lr /= 10\n",
        "  if epoch >= 137:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0,150):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  print('\\n경과 시간:', time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Total average train accuracy: 0.37112\n",
            "Total average train loss: 0.0130403590965271\n",
            "\n",
            " Test epoch: 0\n",
            "\n",
            "Total average test accuarcy: 0.4791\n",
            "Total average test loss: 0.014729096579551697\n",
            "\n",
            "경과 시간: 16.607301712036133\n",
            "Epoch: 1\n",
            "\n",
            "Total average train accuracy: 0.58458\n",
            "Total average train loss: 0.008958596982955933\n",
            "\n",
            " Test epoch: 1\n",
            "\n",
            "Total average test accuarcy: 0.5178\n",
            "Total average test loss: 0.01462576711177826\n",
            "\n",
            "경과 시간: 33.1825065612793\n",
            "Epoch: 2\n",
            "\n",
            "Total average train accuracy: 0.66898\n",
            "Total average train loss: 0.007254592483043671\n",
            "\n",
            " Test epoch: 2\n",
            "\n",
            "Total average test accuarcy: 0.6591\n",
            "Total average test loss: 0.009767863124608993\n",
            "\n",
            "경과 시간: 49.70162224769592\n",
            "Epoch: 3\n",
            "\n",
            "Total average train accuracy: 0.7249\n",
            "Total average train loss: 0.006134643958806992\n",
            "\n",
            " Test epoch: 3\n",
            "\n",
            "Total average test accuarcy: 0.7257\n",
            "Total average test loss: 0.007969421565532684\n",
            "\n",
            "경과 시간: 66.3463351726532\n",
            "Epoch: 4\n",
            "\n",
            "Total average train accuracy: 0.76084\n",
            "Total average train loss: 0.005370581101775169\n",
            "\n",
            " Test epoch: 4\n",
            "\n",
            "Total average test accuarcy: 0.692\n",
            "Total average test loss: 0.009806100165843963\n",
            "\n",
            "경과 시간: 82.96116924285889\n",
            "Epoch: 5\n",
            "\n",
            "Total average train accuracy: 0.78442\n",
            "Total average train loss: 0.00486655904352665\n",
            "\n",
            " Test epoch: 5\n",
            "\n",
            "Total average test accuarcy: 0.7705\n",
            "Total average test loss: 0.007023631975054741\n",
            "\n",
            "경과 시간: 99.57688403129578\n",
            "Epoch: 6\n",
            "\n",
            "Total average train accuracy: 0.79688\n",
            "Total average train loss: 0.004553177921175957\n",
            "\n",
            " Test epoch: 6\n",
            "\n",
            "Total average test accuarcy: 0.7816\n",
            "Total average test loss: 0.00657568279504776\n",
            "\n",
            "경과 시간: 116.1600251197815\n",
            "Epoch: 7\n",
            "\n",
            "Total average train accuracy: 0.81356\n",
            "Total average train loss: 0.004197874046564102\n",
            "\n",
            " Test epoch: 7\n",
            "\n",
            "Total average test accuarcy: 0.76\n",
            "Total average test loss: 0.007337771219015122\n",
            "\n",
            "경과 시간: 132.8324098587036\n",
            "Epoch: 8\n",
            "\n",
            "Total average train accuracy: 0.82394\n",
            "Total average train loss: 0.003997464318871498\n",
            "\n",
            " Test epoch: 8\n",
            "\n",
            "Total average test accuarcy: 0.8041\n",
            "Total average test loss: 0.005948661950230598\n",
            "\n",
            "경과 시간: 149.4694139957428\n",
            "Epoch: 9\n",
            "\n",
            "Total average train accuracy: 0.83128\n",
            "Total average train loss: 0.0038267676252126694\n",
            "\n",
            " Test epoch: 9\n",
            "\n",
            "Total average test accuarcy: 0.8162\n",
            "Total average test loss: 0.005445945248007775\n",
            "\n",
            "경과 시간: 165.96251153945923\n",
            "Epoch: 10\n",
            "\n",
            "Total average train accuracy: 0.83786\n",
            "Total average train loss: 0.0036521290612220765\n",
            "\n",
            " Test epoch: 10\n",
            "\n",
            "Total average test accuarcy: 0.7684\n",
            "Total average test loss: 0.007539900428056717\n",
            "\n",
            "경과 시간: 182.4530544281006\n",
            "Epoch: 11\n",
            "\n",
            "Total average train accuracy: 0.8437\n",
            "Total average train loss: 0.0035311212301254273\n",
            "\n",
            " Test epoch: 11\n",
            "\n",
            "Total average test accuarcy: 0.8327\n",
            "Total average test loss: 0.005164548948407173\n",
            "\n",
            "경과 시간: 198.99994826316833\n",
            "Epoch: 12\n",
            "\n",
            "Total average train accuracy: 0.84528\n",
            "Total average train loss: 0.0034502273574471476\n",
            "\n",
            " Test epoch: 12\n",
            "\n",
            "Total average test accuarcy: 0.8061\n",
            "Total average test loss: 0.0056732998341321944\n",
            "\n",
            "경과 시간: 215.61461329460144\n",
            "Epoch: 13\n",
            "\n",
            "Total average train accuracy: 0.85524\n",
            "Total average train loss: 0.0032880567598342894\n",
            "\n",
            " Test epoch: 13\n",
            "\n",
            "Total average test accuarcy: 0.8149\n",
            "Total average test loss: 0.005680441629886627\n",
            "\n",
            "경과 시간: 232.30505466461182\n",
            "Epoch: 14\n",
            "\n",
            "Total average train accuracy: 0.85572\n",
            "Total average train loss: 0.003204762903749943\n",
            "\n",
            " Test epoch: 14\n",
            "\n",
            "Total average test accuarcy: 0.8105\n",
            "Total average test loss: 0.005700245407223701\n",
            "\n",
            "경과 시간: 248.86145496368408\n",
            "Epoch: 15\n",
            "\n",
            "Total average train accuracy: 0.8614\n",
            "Total average train loss: 0.0031452413427829743\n",
            "\n",
            " Test epoch: 15\n",
            "\n",
            "Total average test accuarcy: 0.8081\n",
            "Total average test loss: 0.005838887882232666\n",
            "\n",
            "경과 시간: 265.4781401157379\n",
            "Epoch: 16\n",
            "\n",
            "Total average train accuracy: 0.86482\n",
            "Total average train loss: 0.0030604709315299986\n",
            "\n",
            " Test epoch: 16\n",
            "\n",
            "Total average test accuarcy: 0.8158\n",
            "Total average test loss: 0.005725296375155449\n",
            "\n",
            "경과 시간: 282.04756021499634\n",
            "Epoch: 17\n",
            "\n",
            "Total average train accuracy: 0.86766\n",
            "Total average train loss: 0.002974416478872299\n",
            "\n",
            " Test epoch: 17\n",
            "\n",
            "Total average test accuarcy: 0.8229\n",
            "Total average test loss: 0.005275504994392395\n",
            "\n",
            "경과 시간: 298.63577032089233\n",
            "Epoch: 18\n",
            "\n",
            "Total average train accuracy: 0.87126\n",
            "Total average train loss: 0.0028821358373761177\n",
            "\n",
            " Test epoch: 18\n",
            "\n",
            "Total average test accuarcy: 0.8315\n",
            "Total average test loss: 0.004751622748374939\n",
            "\n",
            "경과 시간: 315.3062357902527\n",
            "Epoch: 19\n",
            "\n",
            "Total average train accuracy: 0.87398\n",
            "Total average train loss: 0.0028484044605493544\n",
            "\n",
            " Test epoch: 19\n",
            "\n",
            "Total average test accuarcy: 0.8105\n",
            "Total average test loss: 0.005878552979230881\n",
            "\n",
            "경과 시간: 331.7947506904602\n",
            "Epoch: 20\n",
            "\n",
            "Total average train accuracy: 0.87514\n",
            "Total average train loss: 0.00279677106410265\n",
            "\n",
            " Test epoch: 20\n",
            "\n",
            "Total average test accuarcy: 0.8418\n",
            "Total average test loss: 0.004735628804564476\n",
            "\n",
            "경과 시간: 348.416531085968\n",
            "Epoch: 21\n",
            "\n",
            "Total average train accuracy: 0.8773\n",
            "Total average train loss: 0.002746334799230099\n",
            "\n",
            " Test epoch: 21\n",
            "\n",
            "Total average test accuarcy: 0.8352\n",
            "Total average test loss: 0.005181572182476521\n",
            "\n",
            "경과 시간: 365.00049901008606\n",
            "Epoch: 22\n",
            "\n",
            "Total average train accuracy: 0.87838\n",
            "Total average train loss: 0.0027277034401893617\n",
            "\n",
            " Test epoch: 22\n",
            "\n",
            "Total average test accuarcy: 0.8237\n",
            "Total average test loss: 0.005510806819796562\n",
            "\n",
            "경과 시간: 381.7561585903168\n",
            "Epoch: 23\n",
            "\n",
            "Total average train accuracy: 0.88214\n",
            "Total average train loss: 0.002634723207652569\n",
            "\n",
            " Test epoch: 23\n",
            "\n",
            "Total average test accuarcy: 0.843\n",
            "Total average test loss: 0.004834529201686383\n",
            "\n",
            "경과 시간: 398.31478571891785\n",
            "Epoch: 24\n",
            "\n",
            "Total average train accuracy: 0.88508\n",
            "Total average train loss: 0.002602670061290264\n",
            "\n",
            " Test epoch: 24\n",
            "\n",
            "Total average test accuarcy: 0.84\n",
            "Total average test loss: 0.005038997229933739\n",
            "\n",
            "경과 시간: 414.8962993621826\n",
            "Epoch: 25\n",
            "\n",
            "Total average train accuracy: 0.88836\n",
            "Total average train loss: 0.002524158918261528\n",
            "\n",
            " Test epoch: 25\n",
            "\n",
            "Total average test accuarcy: 0.8308\n",
            "Total average test loss: 0.005095506483316421\n",
            "\n",
            "경과 시간: 431.4914400577545\n",
            "Epoch: 26\n",
            "\n",
            "Total average train accuracy: 0.8891\n",
            "Total average train loss: 0.0024937225142121315\n",
            "\n",
            " Test epoch: 26\n",
            "\n",
            "Total average test accuarcy: 0.8251\n",
            "Total average test loss: 0.005391344922780991\n",
            "\n",
            "경과 시간: 448.18224596977234\n",
            "Epoch: 27\n",
            "\n",
            "Total average train accuracy: 0.89014\n",
            "Total average train loss: 0.0024628944262862205\n",
            "\n",
            " Test epoch: 27\n",
            "\n",
            "Total average test accuarcy: 0.8216\n",
            "Total average test loss: 0.005580857172608375\n",
            "\n",
            "경과 시간: 464.7637794017792\n",
            "Epoch: 28\n",
            "\n",
            "Total average train accuracy: 0.88976\n",
            "Total average train loss: 0.0024633614587783815\n",
            "\n",
            " Test epoch: 28\n",
            "\n",
            "Total average test accuarcy: 0.8339\n",
            "Total average test loss: 0.005158800646662712\n",
            "\n",
            "경과 시간: 481.3829164505005\n",
            "Epoch: 29\n",
            "\n",
            "Total average train accuracy: 0.8933\n",
            "Total average train loss: 0.0024010677230358124\n",
            "\n",
            " Test epoch: 29\n",
            "\n",
            "Total average test accuarcy: 0.8554\n",
            "Total average test loss: 0.0043585960403084755\n",
            "\n",
            "경과 시간: 498.1456289291382\n",
            "Epoch: 30\n",
            "\n",
            "Total average train accuracy: 0.8946\n",
            "Total average train loss: 0.0023483686915040016\n",
            "\n",
            " Test epoch: 30\n",
            "\n",
            "Total average test accuarcy: 0.8382\n",
            "Total average test loss: 0.005200357523560524\n",
            "\n",
            "경과 시간: 514.7924611568451\n",
            "Epoch: 31\n",
            "\n",
            "Total average train accuracy: 0.89354\n",
            "Total average train loss: 0.0023632909041643143\n",
            "\n",
            " Test epoch: 31\n",
            "\n",
            "Total average test accuarcy: 0.8311\n",
            "Total average test loss: 0.005794399398565292\n",
            "\n",
            "경과 시간: 531.3013033866882\n",
            "Epoch: 32\n",
            "\n",
            "Total average train accuracy: 0.89402\n",
            "Total average train loss: 0.0023392264741659164\n",
            "\n",
            " Test epoch: 32\n",
            "\n",
            "Total average test accuarcy: 0.8397\n",
            "Total average test loss: 0.0050217533439397815\n",
            "\n",
            "경과 시간: 547.9636914730072\n",
            "Epoch: 33\n",
            "\n",
            "Total average train accuracy: 0.89604\n",
            "Total average train loss: 0.002324857553243637\n",
            "\n",
            " Test epoch: 33\n",
            "\n",
            "Total average test accuarcy: 0.8256\n",
            "Total average test loss: 0.0055948872059583665\n",
            "\n",
            "경과 시간: 564.6947708129883\n",
            "Epoch: 34\n",
            "\n",
            "Total average train accuracy: 0.8981\n",
            "Total average train loss: 0.0023062977889180183\n",
            "\n",
            " Test epoch: 34\n",
            "\n",
            "Total average test accuarcy: 0.8424\n",
            "Total average test loss: 0.004906346273422241\n",
            "\n",
            "경과 시간: 581.3332250118256\n",
            "Epoch: 35\n",
            "\n",
            "Total average train accuracy: 0.90068\n",
            "Total average train loss: 0.002230787031650543\n",
            "\n",
            " Test epoch: 35\n",
            "\n",
            "Total average test accuarcy: 0.8343\n",
            "Total average test loss: 0.0053704683840274815\n",
            "\n",
            "경과 시간: 598.081494808197\n",
            "Epoch: 36\n",
            "\n",
            "Total average train accuracy: 0.89978\n",
            "Total average train loss: 0.002250116862207651\n",
            "\n",
            " Test epoch: 36\n",
            "\n",
            "Total average test accuarcy: 0.8439\n",
            "Total average test loss: 0.004842463946342468\n",
            "\n",
            "경과 시간: 614.6611866950989\n",
            "Epoch: 37\n",
            "\n",
            "Total average train accuracy: 0.90012\n",
            "Total average train loss: 0.0022197855082154274\n",
            "\n",
            " Test epoch: 37\n",
            "\n",
            "Total average test accuarcy: 0.8423\n",
            "Total average test loss: 0.004877035488188267\n",
            "\n",
            "경과 시간: 631.2855944633484\n",
            "Epoch: 38\n",
            "\n",
            "Total average train accuracy: 0.90034\n",
            "Total average train loss: 0.002222828339934349\n",
            "\n",
            " Test epoch: 38\n",
            "\n",
            "Total average test accuarcy: 0.8361\n",
            "Total average test loss: 0.005188363930583\n",
            "\n",
            "경과 시간: 647.8190684318542\n",
            "Epoch: 39\n",
            "\n",
            "Total average train accuracy: 0.90354\n",
            "Total average train loss: 0.0021705809596180917\n",
            "\n",
            " Test epoch: 39\n",
            "\n",
            "Total average test accuarcy: 0.8591\n",
            "Total average test loss: 0.0044241477549076084\n",
            "\n",
            "경과 시간: 664.3726115226746\n",
            "Epoch: 40\n",
            "\n",
            "Total average train accuracy: 0.90264\n",
            "Total average train loss: 0.0021566476741433143\n",
            "\n",
            " Test epoch: 40\n",
            "\n",
            "Total average test accuarcy: 0.8545\n",
            "Total average test loss: 0.004839234375953674\n",
            "\n",
            "경과 시간: 680.962409734726\n",
            "Epoch: 41\n",
            "\n",
            "Total average train accuracy: 0.9027\n",
            "Total average train loss: 0.002183933248370886\n",
            "\n",
            " Test epoch: 41\n",
            "\n",
            "Total average test accuarcy: 0.8395\n",
            "Total average test loss: 0.005089682260155678\n",
            "\n",
            "경과 시간: 697.4638640880585\n",
            "Epoch: 42\n",
            "\n",
            "Total average train accuracy: 0.90552\n",
            "Total average train loss: 0.002104177277982235\n",
            "\n",
            " Test epoch: 42\n",
            "\n",
            "Total average test accuarcy: 0.8586\n",
            "Total average test loss: 0.004456852778792381\n",
            "\n",
            "경과 시간: 714.1614143848419\n",
            "Epoch: 43\n",
            "\n",
            "Total average train accuracy: 0.90408\n",
            "Total average train loss: 0.0021186396104097365\n",
            "\n",
            " Test epoch: 43\n",
            "\n",
            "Total average test accuarcy: 0.8582\n",
            "Total average test loss: 0.004362592799961567\n",
            "\n",
            "경과 시간: 730.7624337673187\n",
            "Epoch: 44\n",
            "\n",
            "Total average train accuracy: 0.90484\n",
            "Total average train loss: 0.002089582108259201\n",
            "\n",
            " Test epoch: 44\n",
            "\n",
            "Total average test accuarcy: 0.8524\n",
            "Total average test loss: 0.004876274716854095\n",
            "\n",
            "경과 시간: 747.5416896343231\n",
            "Epoch: 45\n",
            "\n",
            "Total average train accuracy: 0.9067\n",
            "Total average train loss: 0.002095752659142017\n",
            "\n",
            " Test epoch: 45\n",
            "\n",
            "Total average test accuarcy: 0.7605\n",
            "Total average test loss: 0.009828122729063034\n",
            "\n",
            "경과 시간: 764.6013941764832\n",
            "Epoch: 46\n",
            "\n",
            "Total average train accuracy: 0.90532\n",
            "Total average train loss: 0.0020792612347006797\n",
            "\n",
            " Test epoch: 46\n",
            "\n",
            "Total average test accuarcy: 0.8662\n",
            "Total average test loss: 0.004035365809500218\n",
            "\n",
            "경과 시간: 781.6553781032562\n",
            "Epoch: 47\n",
            "\n",
            "Total average train accuracy: 0.90734\n",
            "Total average train loss: 0.0020731575495004655\n",
            "\n",
            " Test epoch: 47\n",
            "\n",
            "Total average test accuarcy: 0.8415\n",
            "Total average test loss: 0.0048885234907269474\n",
            "\n",
            "경과 시간: 798.5295326709747\n",
            "Epoch: 48\n",
            "\n",
            "Total average train accuracy: 0.90972\n",
            "Total average train loss: 0.0020171963775157927\n",
            "\n",
            " Test epoch: 48\n",
            "\n",
            "Total average test accuarcy: 0.8686\n",
            "Total average test loss: 0.00420238706022501\n",
            "\n",
            "경과 시간: 815.1249313354492\n",
            "Epoch: 49\n",
            "\n",
            "Total average train accuracy: 0.90706\n",
            "Total average train loss: 0.0020456370389461517\n",
            "\n",
            " Test epoch: 49\n",
            "\n",
            "Total average test accuarcy: 0.8691\n",
            "Total average test loss: 0.004144829942286015\n",
            "\n",
            "경과 시간: 831.755294084549\n",
            "Epoch: 50\n",
            "\n",
            "Total average train accuracy: 0.91134\n",
            "Total average train loss: 0.001986354171484709\n",
            "\n",
            " Test epoch: 50\n",
            "\n",
            "Total average test accuarcy: 0.8684\n",
            "Total average test loss: 0.00412571369856596\n",
            "\n",
            "경과 시간: 848.5801687240601\n",
            "Epoch: 51\n",
            "\n",
            "Total average train accuracy: 0.9104\n",
            "Total average train loss: 0.001987036534845829\n",
            "\n",
            " Test epoch: 51\n",
            "\n",
            "Total average test accuarcy: 0.839\n",
            "Total average test loss: 0.005226672226190567\n",
            "\n",
            "경과 시간: 865.2264547348022\n",
            "Epoch: 52\n",
            "\n",
            "Total average train accuracy: 0.91276\n",
            "Total average train loss: 0.0019493726147711277\n",
            "\n",
            " Test epoch: 52\n",
            "\n",
            "Total average test accuarcy: 0.8511\n",
            "Total average test loss: 0.0048915396630764005\n",
            "\n",
            "경과 시간: 881.8786034584045\n",
            "Epoch: 53\n",
            "\n",
            "Total average train accuracy: 0.91054\n",
            "Total average train loss: 0.001990394092202187\n",
            "\n",
            " Test epoch: 53\n",
            "\n",
            "Total average test accuarcy: 0.8607\n",
            "Total average test loss: 0.004482888120412826\n",
            "\n",
            "경과 시간: 898.4728908538818\n",
            "Epoch: 54\n",
            "\n",
            "Total average train accuracy: 0.91274\n",
            "Total average train loss: 0.001955174018740654\n",
            "\n",
            " Test epoch: 54\n",
            "\n",
            "Total average test accuarcy: 0.8727\n",
            "Total average test loss: 0.004162968321144581\n",
            "\n",
            "경과 시간: 914.9969663619995\n",
            "Epoch: 55\n",
            "\n",
            "Total average train accuracy: 0.91212\n",
            "Total average train loss: 0.0019678426629304887\n",
            "\n",
            " Test epoch: 55\n",
            "\n",
            "Total average test accuarcy: 0.844\n",
            "Total average test loss: 0.0050526850938796995\n",
            "\n",
            "경과 시간: 931.6098845005035\n",
            "Epoch: 56\n",
            "\n",
            "Total average train accuracy: 0.91324\n",
            "Total average train loss: 0.0019378060244023799\n",
            "\n",
            " Test epoch: 56\n",
            "\n",
            "Total average test accuarcy: 0.8502\n",
            "Total average test loss: 0.0046136003464460375\n",
            "\n",
            "경과 시간: 948.2284729480743\n",
            "Epoch: 57\n",
            "\n",
            "Total average train accuracy: 0.9134\n",
            "Total average train loss: 0.0019518052799999714\n",
            "\n",
            " Test epoch: 57\n",
            "\n",
            "Total average test accuarcy: 0.8766\n",
            "Total average test loss: 0.003956476399302483\n",
            "\n",
            "경과 시간: 964.8096706867218\n",
            "Epoch: 58\n",
            "\n",
            "Total average train accuracy: 0.9125\n",
            "Total average train loss: 0.0019277216406166553\n",
            "\n",
            " Test epoch: 58\n",
            "\n",
            "Total average test accuarcy: 0.8763\n",
            "Total average test loss: 0.003961535212397576\n",
            "\n",
            "경과 시간: 981.3943989276886\n",
            "Epoch: 59\n",
            "\n",
            "Total average train accuracy: 0.91194\n",
            "Total average train loss: 0.0019320150455832481\n",
            "\n",
            " Test epoch: 59\n",
            "\n",
            "Total average test accuarcy: 0.8567\n",
            "Total average test loss: 0.004541588304936886\n",
            "\n",
            "경과 시간: 997.9800183773041\n",
            "Epoch: 60\n",
            "\n",
            "Total average train accuracy: 0.91492\n",
            "Total average train loss: 0.0018932823808491231\n",
            "\n",
            " Test epoch: 60\n",
            "\n",
            "Total average test accuarcy: 0.853\n",
            "Total average test loss: 0.004730214369297028\n",
            "\n",
            "경과 시간: 1014.5612213611603\n",
            "Epoch: 61\n",
            "\n",
            "Total average train accuracy: 0.91374\n",
            "Total average train loss: 0.0019089813289046287\n",
            "\n",
            " Test epoch: 61\n",
            "\n",
            "Total average test accuarcy: 0.8557\n",
            "Total average test loss: 0.00481831265091896\n",
            "\n",
            "경과 시간: 1031.2071509361267\n",
            "Epoch: 62\n",
            "\n",
            "Total average train accuracy: 0.91766\n",
            "Total average train loss: 0.001853176577091217\n",
            "\n",
            " Test epoch: 62\n",
            "\n",
            "Total average test accuarcy: 0.8206\n",
            "Total average test loss: 0.006504371392726898\n",
            "\n",
            "경과 시간: 1047.7951745986938\n",
            "Epoch: 63\n",
            "\n",
            "Total average train accuracy: 0.91538\n",
            "Total average train loss: 0.0018750139348208904\n",
            "\n",
            " Test epoch: 63\n",
            "\n",
            "Total average test accuarcy: 0.863\n",
            "Total average test loss: 0.00438205953091383\n",
            "\n",
            "경과 시간: 1064.3838138580322\n",
            "Epoch: 64\n",
            "\n",
            "Total average train accuracy: 0.9156\n",
            "Total average train loss: 0.0018810743068158626\n",
            "\n",
            " Test epoch: 64\n",
            "\n",
            "Total average test accuarcy: 0.8729\n",
            "Total average test loss: 0.004084856785833836\n",
            "\n",
            "경과 시간: 1080.9442620277405\n",
            "Epoch: 65\n",
            "\n",
            "Total average train accuracy: 0.91768\n",
            "Total average train loss: 0.0018428505219519138\n",
            "\n",
            " Test epoch: 65\n",
            "\n",
            "Total average test accuarcy: 0.8775\n",
            "Total average test loss: 0.0038396223649382593\n",
            "\n",
            "경과 시간: 1097.5021803379059\n",
            "Epoch: 66\n",
            "\n",
            "Total average train accuracy: 0.91648\n",
            "Total average train loss: 0.001848576122224331\n",
            "\n",
            " Test epoch: 66\n",
            "\n",
            "Total average test accuarcy: 0.8559\n",
            "Total average test loss: 0.004911580906808376\n",
            "\n",
            "경과 시간: 1114.2364072799683\n",
            "Epoch: 67\n",
            "\n",
            "Total average train accuracy: 0.91638\n",
            "Total average train loss: 0.0018641853612661361\n",
            "\n",
            " Test epoch: 67\n",
            "\n",
            "Total average test accuarcy: 0.8659\n",
            "Total average test loss: 0.004510730192065239\n",
            "\n",
            "경과 시간: 1130.9472482204437\n",
            "Epoch: 68\n",
            "\n",
            "Total average train accuracy: 0.91816\n",
            "Total average train loss: 0.0018257851146161557\n",
            "\n",
            " Test epoch: 68\n",
            "\n",
            "Total average test accuarcy: 0.8648\n",
            "Total average test loss: 0.004346755197644233\n",
            "\n",
            "경과 시간: 1147.535048007965\n",
            "Epoch: 69\n",
            "\n",
            "Total average train accuracy: 0.91588\n",
            "Total average train loss: 0.0018479874208569527\n",
            "\n",
            " Test epoch: 69\n",
            "\n",
            "Total average test accuarcy: 0.8534\n",
            "Total average test loss: 0.0045488334938883785\n",
            "\n",
            "경과 시간: 1164.0854666233063\n",
            "Epoch: 70\n",
            "\n",
            "Total average train accuracy: 0.91908\n",
            "Total average train loss: 0.001820500950962305\n",
            "\n",
            " Test epoch: 70\n",
            "\n",
            "Total average test accuarcy: 0.8512\n",
            "Total average test loss: 0.004713209129869938\n",
            "\n",
            "경과 시간: 1180.6598603725433\n",
            "Epoch: 71\n",
            "\n",
            "Total average train accuracy: 0.91766\n",
            "Total average train loss: 0.0018452992302924395\n",
            "\n",
            " Test epoch: 71\n",
            "\n",
            "Total average test accuarcy: 0.8605\n",
            "Total average test loss: 0.004336544609069824\n",
            "\n",
            "경과 시간: 1197.3680775165558\n",
            "Epoch: 72\n",
            "\n",
            "Total average train accuracy: 0.9184\n",
            "Total average train loss: 0.0017926738311350345\n",
            "\n",
            " Test epoch: 72\n",
            "\n",
            "Total average test accuarcy: 0.8747\n",
            "Total average test loss: 0.004235043147206306\n",
            "\n",
            "경과 시간: 1213.922292470932\n",
            "Epoch: 73\n",
            "\n",
            "Total average train accuracy: 0.91804\n",
            "Total average train loss: 0.0018152814714610577\n",
            "\n",
            " Test epoch: 73\n",
            "\n",
            "Total average test accuarcy: 0.8464\n",
            "Total average test loss: 0.0051752481088042255\n",
            "\n",
            "경과 시간: 1230.486246585846\n",
            "Epoch: 74\n",
            "\n",
            "Total average train accuracy: 0.91828\n",
            "Total average train loss: 0.0018188779608905316\n",
            "\n",
            " Test epoch: 74\n",
            "\n",
            "Total average test accuarcy: 0.8565\n",
            "Total average test loss: 0.004585114227235317\n",
            "\n",
            "경과 시간: 1247.0104920864105\n",
            "Epoch: 75\n",
            "\n",
            "Total average train accuracy: 0.91834\n",
            "Total average train loss: 0.001816851772516966\n",
            "\n",
            " Test epoch: 75\n",
            "\n",
            "Total average test accuarcy: 0.8464\n",
            "Total average test loss: 0.005088512253761291\n",
            "\n",
            "경과 시간: 1263.5930495262146\n",
            "Epoch: 76\n",
            "\n",
            "Total average train accuracy: 0.91894\n",
            "Total average train loss: 0.0018030265529453754\n",
            "\n",
            " Test epoch: 76\n",
            "\n",
            "Total average test accuarcy: 0.8519\n",
            "Total average test loss: 0.005226915560662747\n",
            "\n",
            "경과 시간: 1280.1794648170471\n",
            "Epoch: 77\n",
            "\n",
            "Total average train accuracy: 0.919\n",
            "Total average train loss: 0.0017997494777292013\n",
            "\n",
            " Test epoch: 77\n",
            "\n",
            "Total average test accuarcy: 0.866\n",
            "Total average test loss: 0.004308609409630299\n",
            "\n",
            "경과 시간: 1296.8457534313202\n",
            "Epoch: 78\n",
            "\n",
            "Total average train accuracy: 0.91884\n",
            "Total average train loss: 0.0017887896551191807\n",
            "\n",
            " Test epoch: 78\n",
            "\n",
            "Total average test accuarcy: 0.8678\n",
            "Total average test loss: 0.004181764186918736\n",
            "\n",
            "경과 시간: 1313.8582005500793\n",
            "Epoch: 79\n",
            "\n",
            "Total average train accuracy: 0.91908\n",
            "Total average train loss: 0.0017972111003100872\n",
            "\n",
            " Test epoch: 79\n",
            "\n",
            "Total average test accuarcy: 0.8745\n",
            "Total average test loss: 0.004006112349033356\n",
            "\n",
            "경과 시간: 1331.1915113925934\n",
            "Epoch: 80\n",
            "\n",
            "Total average train accuracy: 0.92138\n",
            "Total average train loss: 0.0017587561988830567\n",
            "\n",
            " Test epoch: 80\n",
            "\n",
            "Total average test accuarcy: 0.8487\n",
            "Total average test loss: 0.004967216593027115\n",
            "\n",
            "경과 시간: 1348.4356906414032\n",
            "Epoch: 81\n",
            "\n",
            "Total average train accuracy: 0.91988\n",
            "Total average train loss: 0.0017993664649128913\n",
            "\n",
            " Test epoch: 81\n",
            "\n",
            "Total average test accuarcy: 0.8699\n",
            "Total average test loss: 0.0041727489605546\n",
            "\n",
            "경과 시간: 1365.3099472522736\n",
            "Epoch: 82\n",
            "\n",
            "Total average train accuracy: 0.9208\n",
            "Total average train loss: 0.001778274925351143\n",
            "\n",
            " Test epoch: 82\n",
            "\n",
            "Total average test accuarcy: 0.8655\n",
            "Total average test loss: 0.004354756027460098\n",
            "\n",
            "경과 시간: 1382.014473438263\n",
            "Epoch: 83\n",
            "\n",
            "Total average train accuracy: 0.92244\n",
            "Total average train loss: 0.0017363539054989815\n",
            "\n",
            " Test epoch: 83\n",
            "\n",
            "Total average test accuarcy: 0.8444\n",
            "Total average test loss: 0.005308000901341438\n",
            "\n",
            "경과 시간: 1398.7637791633606\n",
            "Epoch: 84\n",
            "\n",
            "Total average train accuracy: 0.92024\n",
            "Total average train loss: 0.0017452975599467754\n",
            "\n",
            " Test epoch: 84\n",
            "\n",
            "Total average test accuarcy: 0.8368\n",
            "Total average test loss: 0.005655615931749344\n",
            "\n",
            "경과 시간: 1415.5772726535797\n",
            "Epoch: 85\n",
            "\n",
            "Total average train accuracy: 0.92208\n",
            "Total average train loss: 0.001749011573344469\n",
            "\n",
            " Test epoch: 85\n",
            "\n",
            "Total average test accuarcy: 0.8416\n",
            "Total average test loss: 0.0052427559822797775\n",
            "\n",
            "경과 시간: 1432.4563145637512\n",
            "Epoch: 86\n",
            "\n",
            "Total average train accuracy: 0.92008\n",
            "Total average train loss: 0.0017589992758631707\n",
            "\n",
            " Test epoch: 86\n",
            "\n",
            "Total average test accuarcy: 0.8578\n",
            "Total average test loss: 0.0047793167918920515\n",
            "\n",
            "경과 시간: 1449.202618598938\n",
            "Epoch: 87\n",
            "\n",
            "Total average train accuracy: 0.92228\n",
            "Total average train loss: 0.0017172586578130722\n",
            "\n",
            " Test epoch: 87\n",
            "\n",
            "Total average test accuarcy: 0.8648\n",
            "Total average test loss: 0.004200355552136898\n",
            "\n",
            "경과 시간: 1465.8392276763916\n",
            "Epoch: 88\n",
            "\n",
            "Total average train accuracy: 0.92218\n",
            "Total average train loss: 0.0017280159506201744\n",
            "\n",
            " Test epoch: 88\n",
            "\n",
            "Total average test accuarcy: 0.8619\n",
            "Total average test loss: 0.004322326847910881\n",
            "\n",
            "경과 시간: 1482.457512140274\n",
            "Epoch: 89\n",
            "\n",
            "Total average train accuracy: 0.92398\n",
            "Total average train loss: 0.0017139616651833057\n",
            "\n",
            " Test epoch: 89\n",
            "\n",
            "Total average test accuarcy: 0.8662\n",
            "Total average test loss: 0.004318018544465303\n",
            "\n",
            "경과 시간: 1499.153700351715\n",
            "Epoch: 90\n",
            "\n",
            "Total average train accuracy: 0.95222\n",
            "Total average train loss: 0.001103385920599103\n",
            "\n",
            " Test epoch: 90\n",
            "\n",
            "Total average test accuarcy: 0.9097\n",
            "Total average test loss: 0.0028726803272962572\n",
            "\n",
            "경과 시간: 1515.780368089676\n",
            "Epoch: 91\n",
            "\n",
            "Total average train accuracy: 0.96362\n",
            "Total average train loss: 0.0008465696682780981\n",
            "\n",
            " Test epoch: 91\n",
            "\n",
            "Total average test accuarcy: 0.9126\n",
            "Total average test loss: 0.002776416891813278\n",
            "\n",
            "경과 시간: 1532.4680624008179\n",
            "Epoch: 92\n",
            "\n",
            "Total average train accuracy: 0.96664\n",
            "Total average train loss: 0.0007722619025781751\n",
            "\n",
            " Test epoch: 92\n",
            "\n",
            "Total average test accuarcy: 0.9118\n",
            "Total average test loss: 0.0028270271196961403\n",
            "\n",
            "경과 시간: 1549.1571154594421\n",
            "Epoch: 93\n",
            "\n",
            "Total average train accuracy: 0.96966\n",
            "Total average train loss: 0.0007134475762024522\n",
            "\n",
            " Test epoch: 93\n",
            "\n",
            "Total average test accuarcy: 0.9122\n",
            "Total average test loss: 0.002820900712162256\n",
            "\n",
            "경과 시간: 1566.065408706665\n",
            "Epoch: 94\n",
            "\n",
            "Total average train accuracy: 0.97056\n",
            "Total average train loss: 0.0006748634862154722\n",
            "\n",
            " Test epoch: 94\n",
            "\n",
            "Total average test accuarcy: 0.9116\n",
            "Total average test loss: 0.0028975937098264693\n",
            "\n",
            "경과 시간: 1582.8748226165771\n",
            "Epoch: 95\n",
            "\n",
            "Total average train accuracy: 0.97226\n",
            "Total average train loss: 0.0006310077809169889\n",
            "\n",
            " Test epoch: 95\n",
            "\n",
            "Total average test accuarcy: 0.9131\n",
            "Total average test loss: 0.002912983300536871\n",
            "\n",
            "경과 시간: 1599.7686944007874\n",
            "Epoch: 96\n",
            "\n",
            "Total average train accuracy: 0.97496\n",
            "Total average train loss: 0.0005920960664190352\n",
            "\n",
            " Test epoch: 96\n",
            "\n",
            "Total average test accuarcy: 0.9127\n",
            "Total average test loss: 0.0029757124848663807\n",
            "\n",
            "경과 시간: 1617.0531928539276\n",
            "Epoch: 97\n",
            "\n",
            "Total average train accuracy: 0.97606\n",
            "Total average train loss: 0.0005660437747091055\n",
            "\n",
            " Test epoch: 97\n",
            "\n",
            "Total average test accuarcy: 0.9137\n",
            "Total average test loss: 0.0030024646040052176\n",
            "\n",
            "경과 시간: 1634.5136363506317\n",
            "Epoch: 98\n",
            "\n",
            "Total average train accuracy: 0.9768\n",
            "Total average train loss: 0.0005441717271134258\n",
            "\n",
            " Test epoch: 98\n",
            "\n",
            "Total average test accuarcy: 0.9109\n",
            "Total average test loss: 0.0030698669925332068\n",
            "\n",
            "경과 시간: 1651.6148562431335\n",
            "Epoch: 99\n",
            "\n",
            "Total average train accuracy: 0.978\n",
            "Total average train loss: 0.0005229427868500352\n",
            "\n",
            " Test epoch: 99\n",
            "\n",
            "Total average test accuarcy: 0.9148\n",
            "Total average test loss: 0.0030123633168637753\n",
            "\n",
            "경과 시간: 1669.108636856079\n",
            "Epoch: 100\n",
            "\n",
            "Total average train accuracy: 0.97746\n",
            "Total average train loss: 0.0005159721728973091\n",
            "\n",
            " Test epoch: 100\n",
            "\n",
            "Total average test accuarcy: 0.9119\n",
            "Total average test loss: 0.0030700014010071755\n",
            "\n",
            "경과 시간: 1686.4278554916382\n",
            "Epoch: 101\n",
            "\n",
            "Total average train accuracy: 0.97922\n",
            "Total average train loss: 0.0005040211879462004\n",
            "\n",
            " Test epoch: 101\n",
            "\n",
            "Total average test accuarcy: 0.9142\n",
            "Total average test loss: 0.003068729192763567\n",
            "\n",
            "경과 시간: 1703.3052780628204\n",
            "Epoch: 102\n",
            "\n",
            "Total average train accuracy: 0.97936\n",
            "Total average train loss: 0.00048792325181886553\n",
            "\n",
            " Test epoch: 102\n",
            "\n",
            "Total average test accuarcy: 0.9134\n",
            "Total average test loss: 0.003183526387065649\n",
            "\n",
            "경과 시간: 1720.126225233078\n",
            "Epoch: 103\n",
            "\n",
            "Total average train accuracy: 0.9806\n",
            "Total average train loss: 0.0004596159009262919\n",
            "\n",
            " Test epoch: 103\n",
            "\n",
            "Total average test accuarcy: 0.9115\n",
            "Total average test loss: 0.0032060657627880572\n",
            "\n",
            "경과 시간: 1737.1867032051086\n",
            "Epoch: 104\n",
            "\n",
            "Total average train accuracy: 0.98054\n",
            "Total average train loss: 0.0004438256982527673\n",
            "\n",
            " Test epoch: 104\n",
            "\n",
            "Total average test accuarcy: 0.9128\n",
            "Total average test loss: 0.003153618860989809\n",
            "\n",
            "경과 시간: 1753.9455649852753\n",
            "Epoch: 105\n",
            "\n",
            "Total average train accuracy: 0.98172\n",
            "Total average train loss: 0.00043867419753223656\n",
            "\n",
            " Test epoch: 105\n",
            "\n",
            "Total average test accuarcy: 0.9145\n",
            "Total average test loss: 0.0031785794034600258\n",
            "\n",
            "경과 시간: 1770.6904158592224\n",
            "Epoch: 106\n",
            "\n",
            "Total average train accuracy: 0.98184\n",
            "Total average train loss: 0.00042735732609406116\n",
            "\n",
            " Test epoch: 106\n",
            "\n",
            "Total average test accuarcy: 0.9142\n",
            "Total average test loss: 0.0031987119399011135\n",
            "\n",
            "경과 시간: 1787.2374396324158\n",
            "Epoch: 107\n",
            "\n",
            "Total average train accuracy: 0.98234\n",
            "Total average train loss: 0.00042142278074286876\n",
            "\n",
            " Test epoch: 107\n",
            "\n",
            "Total average test accuarcy: 0.9133\n",
            "Total average test loss: 0.003232711187750101\n",
            "\n",
            "경과 시간: 1804.0145823955536\n",
            "Epoch: 108\n",
            "\n",
            "Total average train accuracy: 0.9825\n",
            "Total average train loss: 0.0004080954593233764\n",
            "\n",
            " Test epoch: 108\n",
            "\n",
            "Total average test accuarcy: 0.9122\n",
            "Total average test loss: 0.003280171875655651\n",
            "\n",
            "경과 시간: 1820.6751911640167\n",
            "Epoch: 109\n",
            "\n",
            "Total average train accuracy: 0.98292\n",
            "Total average train loss: 0.0004003682101145387\n",
            "\n",
            " Test epoch: 109\n",
            "\n",
            "Total average test accuarcy: 0.9126\n",
            "Total average test loss: 0.0032322096891701222\n",
            "\n",
            "경과 시간: 1837.3483970165253\n",
            "Epoch: 110\n",
            "\n",
            "Total average train accuracy: 0.98262\n",
            "Total average train loss: 0.00040191529171541336\n",
            "\n",
            " Test epoch: 110\n",
            "\n",
            "Total average test accuarcy: 0.9158\n",
            "Total average test loss: 0.003248639962822199\n",
            "\n",
            "경과 시간: 1854.0103070735931\n",
            "Epoch: 111\n",
            "\n",
            "Total average train accuracy: 0.9855\n",
            "Total average train loss: 0.0003617318865843117\n",
            "\n",
            " Test epoch: 111\n",
            "\n",
            "Total average test accuarcy: 0.9131\n",
            "Total average test loss: 0.003376048885285854\n",
            "\n",
            "경과 시간: 1870.6440608501434\n",
            "Epoch: 112\n",
            "\n",
            "Total average train accuracy: 0.9839\n",
            "Total average train loss: 0.00037169396333396434\n",
            "\n",
            " Test epoch: 112\n",
            "\n",
            "Total average test accuarcy: 0.912\n",
            "Total average test loss: 0.003402941431850195\n",
            "\n",
            "경과 시간: 1887.2890062332153\n",
            "Epoch: 113\n",
            "\n",
            "Total average train accuracy: 0.98556\n",
            "Total average train loss: 0.00034933758090250196\n",
            "\n",
            " Test epoch: 113\n",
            "\n",
            "Total average test accuarcy: 0.9144\n",
            "Total average test loss: 0.003332401855289936\n",
            "\n",
            "경과 시간: 1903.9025838375092\n",
            "Epoch: 114\n",
            "\n",
            "Total average train accuracy: 0.98594\n",
            "Total average train loss: 0.00034475950849242507\n",
            "\n",
            " Test epoch: 114\n",
            "\n",
            "Total average test accuarcy: 0.9128\n",
            "Total average test loss: 0.003565761909633875\n",
            "\n",
            "경과 시간: 1920.5683526992798\n",
            "Epoch: 115\n",
            "\n",
            "Total average train accuracy: 0.98478\n",
            "Total average train loss: 0.0003535966692119837\n",
            "\n",
            " Test epoch: 115\n",
            "\n",
            "Total average test accuarcy: 0.9117\n",
            "Total average test loss: 0.003427142845839262\n",
            "\n",
            "경과 시간: 1937.1772401332855\n",
            "Epoch: 116\n",
            "\n",
            "Total average train accuracy: 0.98616\n",
            "Total average train loss: 0.00033158828429877756\n",
            "\n",
            " Test epoch: 116\n",
            "\n",
            "Total average test accuarcy: 0.9113\n",
            "Total average test loss: 0.003439256640523672\n",
            "\n",
            "경과 시간: 1953.7438843250275\n",
            "Epoch: 117\n",
            "\n",
            "Total average train accuracy: 0.98608\n",
            "Total average train loss: 0.00032089660519734025\n",
            "\n",
            " Test epoch: 117\n",
            "\n",
            "Total average test accuarcy: 0.9129\n",
            "Total average test loss: 0.0035193713888525963\n",
            "\n",
            "경과 시간: 1970.3119134902954\n",
            "Epoch: 118\n",
            "\n",
            "Total average train accuracy: 0.98672\n",
            "Total average train loss: 0.0003246980467624962\n",
            "\n",
            " Test epoch: 118\n",
            "\n",
            "Total average test accuarcy: 0.9122\n",
            "Total average test loss: 0.0034571392893791198\n",
            "\n",
            "경과 시간: 1986.9083347320557\n",
            "Epoch: 119\n",
            "\n",
            "Total average train accuracy: 0.9872\n",
            "Total average train loss: 0.00030959094336256384\n",
            "\n",
            " Test epoch: 119\n",
            "\n",
            "Total average test accuarcy: 0.9134\n",
            "Total average test loss: 0.003499425293505192\n",
            "\n",
            "경과 시간: 2003.5507967472076\n",
            "Epoch: 120\n",
            "\n",
            "Total average train accuracy: 0.98706\n",
            "Total average train loss: 0.00030849183281883596\n",
            "\n",
            " Test epoch: 120\n",
            "\n",
            "Total average test accuarcy: 0.9103\n",
            "Total average test loss: 0.0035907423455268143\n",
            "\n",
            "경과 시간: 2020.1677248477936\n",
            "Epoch: 121\n",
            "\n",
            "Total average train accuracy: 0.9874\n",
            "Total average train loss: 0.000291463412605226\n",
            "\n",
            " Test epoch: 121\n",
            "\n",
            "Total average test accuarcy: 0.9117\n",
            "Total average test loss: 0.0035551667496562\n",
            "\n",
            "경과 시간: 2036.804475069046\n",
            "Epoch: 122\n",
            "\n",
            "Total average train accuracy: 0.98744\n",
            "Total average train loss: 0.0002998481432814151\n",
            "\n",
            " Test epoch: 122\n",
            "\n",
            "Total average test accuarcy: 0.9129\n",
            "Total average test loss: 0.0036117809861898422\n",
            "\n",
            "경과 시간: 2053.5089795589447\n",
            "Epoch: 123\n",
            "\n",
            "Total average train accuracy: 0.98762\n",
            "Total average train loss: 0.0002893573425989598\n",
            "\n",
            " Test epoch: 123\n",
            "\n",
            "Total average test accuarcy: 0.9079\n",
            "Total average test loss: 0.0037131569907069206\n",
            "\n",
            "경과 시간: 2070.1707532405853\n",
            "Epoch: 124\n",
            "\n",
            "Total average train accuracy: 0.98774\n",
            "Total average train loss: 0.0003024884291924536\n",
            "\n",
            " Test epoch: 124\n",
            "\n",
            "Total average test accuarcy: 0.91\n",
            "Total average test loss: 0.003640054163336754\n",
            "\n",
            "경과 시간: 2086.780975818634\n",
            "Epoch: 125\n",
            "\n",
            "Total average train accuracy: 0.9889\n",
            "Total average train loss: 0.00027518543890677394\n",
            "\n",
            " Test epoch: 125\n",
            "\n",
            "Total average test accuarcy: 0.9132\n",
            "Total average test loss: 0.0036012507356703283\n",
            "\n",
            "경과 시간: 2103.403371810913\n",
            "Epoch: 126\n",
            "\n",
            "Total average train accuracy: 0.98766\n",
            "Total average train loss: 0.00028583864868618545\n",
            "\n",
            " Test epoch: 126\n",
            "\n",
            "Total average test accuarcy: 0.9132\n",
            "Total average test loss: 0.0036629292637109755\n",
            "\n",
            "경과 시간: 2120.1235082149506\n",
            "Epoch: 127\n",
            "\n",
            "Total average train accuracy: 0.98906\n",
            "Total average train loss: 0.00026529683504253625\n",
            "\n",
            " Test epoch: 127\n",
            "\n",
            "Total average test accuarcy: 0.9097\n",
            "Total average test loss: 0.003738239800184965\n",
            "\n",
            "경과 시간: 2136.6987211704254\n",
            "Epoch: 128\n",
            "\n",
            "Total average train accuracy: 0.98774\n",
            "Total average train loss: 0.0002814624056965113\n",
            "\n",
            " Test epoch: 128\n",
            "\n",
            "Total average test accuarcy: 0.9114\n",
            "Total average test loss: 0.003677270370721817\n",
            "\n",
            "경과 시간: 2153.3633415699005\n",
            "Epoch: 129\n",
            "\n",
            "Total average train accuracy: 0.98842\n",
            "Total average train loss: 0.00027590504319407045\n",
            "\n",
            " Test epoch: 129\n",
            "\n",
            "Total average test accuarcy: 0.9126\n",
            "Total average test loss: 0.0036889915347099304\n",
            "\n",
            "경과 시간: 2170.107469558716\n",
            "Epoch: 130\n",
            "\n",
            "Total average train accuracy: 0.98818\n",
            "Total average train loss: 0.0002775864229071885\n",
            "\n",
            " Test epoch: 130\n",
            "\n",
            "Total average test accuarcy: 0.9092\n",
            "Total average test loss: 0.0037656780771911143\n",
            "\n",
            "경과 시간: 2186.715928554535\n",
            "Epoch: 131\n",
            "\n",
            "Total average train accuracy: 0.9889\n",
            "Total average train loss: 0.0002659213998168707\n",
            "\n",
            " Test epoch: 131\n",
            "\n",
            "Total average test accuarcy: 0.9106\n",
            "Total average test loss: 0.0037931352578103542\n",
            "\n",
            "경과 시간: 2203.3596892356873\n",
            "Epoch: 132\n",
            "\n",
            "Total average train accuracy: 0.9894\n",
            "Total average train loss: 0.00025168058042414484\n",
            "\n",
            " Test epoch: 132\n",
            "\n",
            "Total average test accuarcy: 0.9112\n",
            "Total average test loss: 0.003715006338059902\n",
            "\n",
            "경과 시간: 2220.008628845215\n",
            "Epoch: 133\n",
            "\n",
            "Total average train accuracy: 0.9892\n",
            "Total average train loss: 0.0002644649193249643\n",
            "\n",
            " Test epoch: 133\n",
            "\n",
            "Total average test accuarcy: 0.9122\n",
            "Total average test loss: 0.003790597639232874\n",
            "\n",
            "경과 시간: 2236.6815025806427\n",
            "Epoch: 134\n",
            "\n",
            "Total average train accuracy: 0.99044\n",
            "Total average train loss: 0.0002438394913310185\n",
            "\n",
            " Test epoch: 134\n",
            "\n",
            "Total average test accuarcy: 0.9097\n",
            "Total average test loss: 0.003830962084233761\n",
            "\n",
            "경과 시간: 2253.398333787918\n",
            "Epoch: 135\n",
            "\n",
            "Total average train accuracy: 0.98972\n",
            "Total average train loss: 0.0002479884362500161\n",
            "\n",
            " Test epoch: 135\n",
            "\n",
            "Total average test accuarcy: 0.9117\n",
            "Total average test loss: 0.003720152472704649\n",
            "\n",
            "경과 시간: 2269.940893173218\n",
            "Epoch: 136\n",
            "\n",
            "Total average train accuracy: 0.9904\n",
            "Total average train loss: 0.00023595181357581168\n",
            "\n",
            " Test epoch: 136\n",
            "\n",
            "Total average test accuarcy: 0.9113\n",
            "Total average test loss: 0.0038208504669368266\n",
            "\n",
            "경과 시간: 2286.5628383159637\n",
            "Epoch: 137\n",
            "\n",
            "Total average train accuracy: 0.99102\n",
            "Total average train loss: 0.0002140804578177631\n",
            "\n",
            " Test epoch: 137\n",
            "\n",
            "Total average test accuarcy: 0.9127\n",
            "Total average test loss: 0.003678415859490633\n",
            "\n",
            "경과 시간: 2303.1227793693542\n",
            "Epoch: 138\n",
            "\n",
            "Total average train accuracy: 0.99234\n",
            "Total average train loss: 0.00019318881137296557\n",
            "\n",
            " Test epoch: 138\n",
            "\n",
            "Total average test accuarcy: 0.914\n",
            "Total average test loss: 0.0036534123852849008\n",
            "\n",
            "경과 시간: 2319.8429708480835\n",
            "Epoch: 139\n",
            "\n",
            "Total average train accuracy: 0.99278\n",
            "Total average train loss: 0.00019434691837988795\n",
            "\n",
            " Test epoch: 139\n",
            "\n",
            "Total average test accuarcy: 0.9143\n",
            "Total average test loss: 0.0036537428617477415\n",
            "\n",
            "경과 시간: 2336.5299277305603\n",
            "Epoch: 140\n",
            "\n",
            "Total average train accuracy: 0.99332\n",
            "Total average train loss: 0.00018419569104909897\n",
            "\n",
            " Test epoch: 140\n",
            "\n",
            "Total average test accuarcy: 0.9144\n",
            "Total average test loss: 0.003660835177451372\n",
            "\n",
            "경과 시간: 2353.213781118393\n",
            "Epoch: 141\n",
            "\n",
            "Total average train accuracy: 0.99374\n",
            "Total average train loss: 0.0001751083926204592\n",
            "\n",
            " Test epoch: 141\n",
            "\n",
            "Total average test accuarcy: 0.9141\n",
            "Total average test loss: 0.003680916318297386\n",
            "\n",
            "경과 시간: 2369.842344522476\n",
            "Epoch: 142\n",
            "\n",
            "Total average train accuracy: 0.99362\n",
            "Total average train loss: 0.00017426791774574667\n",
            "\n",
            " Test epoch: 142\n",
            "\n",
            "Total average test accuarcy: 0.9153\n",
            "Total average test loss: 0.0036332210399210455\n",
            "\n",
            "경과 시간: 2386.4551322460175\n",
            "Epoch: 143\n",
            "\n",
            "Total average train accuracy: 0.99362\n",
            "Total average train loss: 0.0001715860580885783\n",
            "\n",
            " Test epoch: 143\n",
            "\n",
            "Total average test accuarcy: 0.9151\n",
            "Total average test loss: 0.003647418247908354\n",
            "\n",
            "경과 시간: 2403.181740283966\n",
            "Epoch: 144\n",
            "\n",
            "Total average train accuracy: 0.99314\n",
            "Total average train loss: 0.00017876892956905066\n",
            "\n",
            " Test epoch: 144\n",
            "\n",
            "Total average test accuarcy: 0.9141\n",
            "Total average test loss: 0.0036379846908152105\n",
            "\n",
            "경과 시간: 2419.8396275043488\n",
            "Epoch: 145\n",
            "\n",
            "Total average train accuracy: 0.99418\n",
            "Total average train loss: 0.00015860271791927517\n",
            "\n",
            " Test epoch: 145\n",
            "\n",
            "Total average test accuarcy: 0.9153\n",
            "Total average test loss: 0.0036494722455739973\n",
            "\n",
            "경과 시간: 2436.5048871040344\n",
            "Epoch: 146\n",
            "\n",
            "Total average train accuracy: 0.99422\n",
            "Total average train loss: 0.0001592834953777492\n",
            "\n",
            " Test epoch: 146\n",
            "\n",
            "Total average test accuarcy: 0.9149\n",
            "Total average test loss: 0.003659793983399868\n",
            "\n",
            "경과 시간: 2453.1612186431885\n",
            "Epoch: 147\n",
            "\n",
            "Total average train accuracy: 0.99366\n",
            "Total average train loss: 0.00016826800798531622\n",
            "\n",
            " Test epoch: 147\n",
            "\n",
            "Total average test accuarcy: 0.9153\n",
            "Total average test loss: 0.00365706375092268\n",
            "\n",
            "경과 시간: 2469.754323244095\n",
            "Epoch: 148\n",
            "\n",
            "Total average train accuracy: 0.99384\n",
            "Total average train loss: 0.0001661267472896725\n",
            "\n",
            " Test epoch: 148\n",
            "\n",
            "Total average test accuarcy: 0.9157\n",
            "Total average test loss: 0.0036713097028434278\n",
            "\n",
            "경과 시간: 2486.515040397644\n",
            "Epoch: 149\n",
            "\n",
            "Total average train accuracy: 0.99418\n",
            "Total average train loss: 0.0001636237277649343\n",
            "\n",
            " Test epoch: 149\n",
            "\n",
            "Total average test accuarcy: 0.9156\n",
            "Total average test loss: 0.003665886100381613\n",
            "\n",
            "경과 시간: 2503.1909289360046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeWGtKEInXAF"
      },
      "source": [
        "### ResNet-56 Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE1WkBzy3pHc"
      },
      "source": [
        "def ResNet56():\n",
        "  return ResNet(ResidualBlock, [9,9,9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HII5hVtY4Ezf",
        "collapsed": true,
        "outputId": "76bb1b09-3fd7-4d4a-d3fa-6e73a33a181c"
      },
      "source": [
        "#신경망 선언\n",
        "net = ResNet56()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet50_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/ResNet56-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    # 100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/ResNet56-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint' + file_name)\n",
        "    print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "[2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "16\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "[2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "32\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oVhc6NV8s4d",
        "collapsed": true,
        "outputId": "995d0fb5-7d79-4240-b9cb-ddeddc1c2347"
      },
      "source": [
        "import time\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
        "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
        "  if epoch >=90:\n",
        "    lr /= 10\n",
        "  if epoch >= 137:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0,150):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  print('\\n경과 시간:', time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Total average train accuracy: 0.279\n",
            "Total average train loss: 0.015149434731006623\n",
            "\n",
            " Test epoch: 0\n",
            "\n",
            "Total average test accuarcy: 0.3715\n",
            "Total average test loss: 0.0173614963889122\n",
            "모델이 저장되었습니다\n",
            "\n",
            "경과 시간: 39.779345989227295\n",
            "Epoch: 1\n",
            "\n",
            "Total average train accuracy: 0.41548\n",
            "Total average train loss: 0.012324053344726563\n",
            "\n",
            " Test epoch: 1\n",
            "\n",
            "Total average test accuarcy: 0.4685\n",
            "Total average test loss: 0.01425133080482483\n",
            "\n",
            "경과 시간: 80.5691978931427\n",
            "Epoch: 2\n",
            "\n",
            "Total average train accuracy: 0.52828\n",
            "Total average train loss: 0.010144297090768814\n",
            "\n",
            " Test epoch: 2\n",
            "\n",
            "Total average test accuarcy: 0.55\n",
            "Total average test loss: 0.012869881367683411\n",
            "\n",
            "경과 시간: 122.29723906517029\n",
            "Epoch: 3\n",
            "\n",
            "Total average train accuracy: 0.62408\n",
            "Total average train loss: 0.00820945801615715\n",
            "\n",
            " Test epoch: 3\n",
            "\n",
            "Total average test accuarcy: 0.5901\n",
            "Total average test loss: 0.011495314401388169\n",
            "\n",
            "경과 시간: 163.7708296775818\n",
            "Epoch: 4\n",
            "\n",
            "Total average train accuracy: 0.69466\n",
            "Total average train loss: 0.0067327895081043245\n",
            "\n",
            " Test epoch: 4\n",
            "\n",
            "Total average test accuarcy: 0.691\n",
            "Total average test loss: 0.008904117786884307\n",
            "\n",
            "경과 시간: 205.29669094085693\n",
            "Epoch: 5\n",
            "\n",
            "Total average train accuracy: 0.74218\n",
            "Total average train loss: 0.005748136979341507\n",
            "\n",
            " Test epoch: 5\n",
            "\n",
            "Total average test accuarcy: 0.7128\n",
            "Total average test loss: 0.008558912605047226\n",
            "\n",
            "경과 시간: 246.8164074420929\n",
            "Epoch: 6\n",
            "\n",
            "Total average train accuracy: 0.77406\n",
            "Total average train loss: 0.005073523468971252\n",
            "\n",
            " Test epoch: 6\n",
            "\n",
            "Total average test accuarcy: 0.7463\n",
            "Total average test loss: 0.007458446002006531\n",
            "\n",
            "경과 시간: 288.4119727611542\n",
            "Epoch: 7\n",
            "\n",
            "Total average train accuracy: 0.79658\n",
            "Total average train loss: 0.004607361043095589\n",
            "\n",
            " Test epoch: 7\n",
            "\n",
            "Total average test accuarcy: 0.7715\n",
            "Total average test loss: 0.007011622443795204\n",
            "\n",
            "경과 시간: 329.94892740249634\n",
            "Epoch: 8\n",
            "\n",
            "Total average train accuracy: 0.81344\n",
            "Total average train loss: 0.0042271601963043215\n",
            "\n",
            " Test epoch: 8\n",
            "\n",
            "Total average test accuarcy: 0.8092\n",
            "Total average test loss: 0.005462036114931106\n",
            "\n",
            "경과 시간: 371.5670201778412\n",
            "Epoch: 9\n",
            "\n",
            "Total average train accuracy: 0.82488\n",
            "Total average train loss: 0.003966407012343407\n",
            "\n",
            " Test epoch: 9\n",
            "\n",
            "Total average test accuarcy: 0.8166\n",
            "Total average test loss: 0.005414844593405723\n",
            "\n",
            "경과 시간: 413.06390357017517\n",
            "Epoch: 10\n",
            "\n",
            "Total average train accuracy: 0.83686\n",
            "Total average train loss: 0.0036835809338092804\n",
            "\n",
            " Test epoch: 10\n",
            "\n",
            "Total average test accuarcy: 0.7954\n",
            "Total average test loss: 0.006386699599027634\n",
            "\n",
            "경과 시간: 454.57291984558105\n",
            "Epoch: 11\n",
            "\n",
            "Total average train accuracy: 0.84528\n",
            "Total average train loss: 0.0035137960040569306\n",
            "\n",
            " Test epoch: 11\n",
            "\n",
            "Total average test accuarcy: 0.8191\n",
            "Total average test loss: 0.005382583564519882\n",
            "\n",
            "경과 시간: 496.112033367157\n",
            "Epoch: 12\n",
            "\n",
            "Total average train accuracy: 0.85302\n",
            "Total average train loss: 0.003327196075618267\n",
            "\n",
            " Test epoch: 12\n",
            "\n",
            "Total average test accuarcy: 0.7969\n",
            "Total average test loss: 0.006599988055229187\n",
            "\n",
            "경과 시간: 537.6527438163757\n",
            "Epoch: 13\n",
            "\n",
            "Total average train accuracy: 0.8602\n",
            "Total average train loss: 0.0031737194299697874\n",
            "\n",
            " Test epoch: 13\n",
            "\n",
            "Total average test accuarcy: 0.8389\n",
            "Total average test loss: 0.00477765601426363\n",
            "\n",
            "경과 시간: 579.1651418209076\n",
            "Epoch: 14\n",
            "\n",
            "Total average train accuracy: 0.86516\n",
            "Total average train loss: 0.0030364323541522027\n",
            "\n",
            " Test epoch: 14\n",
            "\n",
            "Total average test accuarcy: 0.8402\n",
            "Total average test loss: 0.004756348983943462\n",
            "\n",
            "경과 시간: 620.6421105861664\n",
            "Epoch: 15\n",
            "\n",
            "Total average train accuracy: 0.87158\n",
            "Total average train loss: 0.0029138915073871614\n",
            "\n",
            " Test epoch: 15\n",
            "\n",
            "Total average test accuarcy: 0.8438\n",
            "Total average test loss: 0.004765497744083405\n",
            "\n",
            "경과 시간: 662.2930955886841\n",
            "Epoch: 16\n",
            "\n",
            "Total average train accuracy: 0.87542\n",
            "Total average train loss: 0.002818605263829231\n",
            "\n",
            " Test epoch: 16\n",
            "\n",
            "Total average test accuarcy: 0.8423\n",
            "Total average test loss: 0.00487587108463049\n",
            "\n",
            "경과 시간: 703.9538614749908\n",
            "Epoch: 17\n",
            "\n",
            "Total average train accuracy: 0.8802\n",
            "Total average train loss: 0.002701768501698971\n",
            "\n",
            " Test epoch: 17\n",
            "\n",
            "Total average test accuarcy: 0.8316\n",
            "Total average test loss: 0.005224914100766182\n",
            "\n",
            "경과 시간: 745.5486671924591\n",
            "Epoch: 18\n",
            "\n",
            "Total average train accuracy: 0.88292\n",
            "Total average train loss: 0.002666832791864872\n",
            "\n",
            " Test epoch: 18\n",
            "\n",
            "Total average test accuarcy: 0.8419\n",
            "Total average test loss: 0.004746860383450985\n",
            "\n",
            "경과 시간: 787.0732922554016\n",
            "Epoch: 19\n",
            "\n",
            "Total average train accuracy: 0.88512\n",
            "Total average train loss: 0.002571702948212624\n",
            "\n",
            " Test epoch: 19\n",
            "\n",
            "Total average test accuarcy: 0.847\n",
            "Total average test loss: 0.0046555916428565975\n",
            "\n",
            "경과 시간: 828.657464504242\n",
            "Epoch: 20\n",
            "\n",
            "Total average train accuracy: 0.88962\n",
            "Total average train loss: 0.002471168052852154\n",
            "\n",
            " Test epoch: 20\n",
            "\n",
            "Total average test accuarcy: 0.8391\n",
            "Total average test loss: 0.004933361558616161\n",
            "\n",
            "경과 시간: 870.1276252269745\n",
            "Epoch: 21\n",
            "\n",
            "Total average train accuracy: 0.89192\n",
            "Total average train loss: 0.002448462632894516\n",
            "\n",
            " Test epoch: 21\n",
            "\n",
            "Total average test accuarcy: 0.8388\n",
            "Total average test loss: 0.005084445698559284\n",
            "\n",
            "경과 시간: 911.6796448230743\n",
            "Epoch: 22\n",
            "\n",
            "Total average train accuracy: 0.89378\n",
            "Total average train loss: 0.0023807477712631227\n",
            "\n",
            " Test epoch: 22\n",
            "\n",
            "Total average test accuarcy: 0.8488\n",
            "Total average test loss: 0.0048914369568228725\n",
            "\n",
            "경과 시간: 953.2780275344849\n",
            "Epoch: 23\n",
            "\n",
            "Total average train accuracy: 0.89726\n",
            "Total average train loss: 0.00230514881581068\n",
            "\n",
            " Test epoch: 23\n",
            "\n",
            "Total average test accuarcy: 0.861\n",
            "Total average test loss: 0.0044443034440279005\n",
            "\n",
            "경과 시간: 994.8284163475037\n",
            "Epoch: 24\n",
            "\n",
            "Total average train accuracy: 0.89932\n",
            "Total average train loss: 0.0022599066710472107\n",
            "\n",
            " Test epoch: 24\n",
            "\n",
            "Total average test accuarcy: 0.8344\n",
            "Total average test loss: 0.0052821924924850465\n",
            "\n",
            "경과 시간: 1036.4179337024689\n",
            "Epoch: 25\n",
            "\n",
            "Total average train accuracy: 0.90182\n",
            "Total average train loss: 0.0021905049657821654\n",
            "\n",
            " Test epoch: 25\n",
            "\n",
            "Total average test accuarcy: 0.8579\n",
            "Total average test loss: 0.004428641851246357\n",
            "\n",
            "경과 시간: 1078.0410516262054\n",
            "Epoch: 26\n",
            "\n",
            "Total average train accuracy: 0.90528\n",
            "Total average train loss: 0.002147489291727543\n",
            "\n",
            " Test epoch: 26\n",
            "\n",
            "Total average test accuarcy: 0.815\n",
            "Total average test loss: 0.0058770423591136935\n",
            "\n",
            "경과 시간: 1119.5177628993988\n",
            "Epoch: 27\n",
            "\n",
            "Total average train accuracy: 0.90496\n",
            "Total average train loss: 0.0021354990243911743\n",
            "\n",
            " Test epoch: 27\n",
            "\n",
            "Total average test accuarcy: 0.8674\n",
            "Total average test loss: 0.004229135127365589\n",
            "\n",
            "경과 시간: 1160.868852853775\n",
            "Epoch: 28\n",
            "\n",
            "Total average train accuracy: 0.90736\n",
            "Total average train loss: 0.002085114817917347\n",
            "\n",
            " Test epoch: 28\n",
            "\n",
            "Total average test accuarcy: 0.8538\n",
            "Total average test loss: 0.004674987772107124\n",
            "\n",
            "경과 시간: 1202.3808653354645\n",
            "Epoch: 29\n",
            "\n",
            "Total average train accuracy: 0.90876\n",
            "Total average train loss: 0.0020304086807370184\n",
            "\n",
            " Test epoch: 29\n",
            "\n",
            "Total average test accuarcy: 0.8312\n",
            "Total average test loss: 0.005415749797224999\n",
            "\n",
            "경과 시간: 1243.775045633316\n",
            "Epoch: 30\n",
            "\n",
            "Total average train accuracy: 0.91162\n",
            "Total average train loss: 0.0020031093679368497\n",
            "\n",
            " Test epoch: 30\n",
            "\n",
            "Total average test accuarcy: 0.8334\n",
            "Total average test loss: 0.0056266111582517625\n",
            "\n",
            "경과 시간: 1285.1539976596832\n",
            "Epoch: 31\n",
            "\n",
            "Total average train accuracy: 0.91364\n",
            "Total average train loss: 0.0019617928621172903\n",
            "\n",
            " Test epoch: 31\n",
            "\n",
            "Total average test accuarcy: 0.8645\n",
            "Total average test loss: 0.004109722153842449\n",
            "\n",
            "경과 시간: 1326.7158901691437\n",
            "Epoch: 32\n",
            "\n",
            "Total average train accuracy: 0.91544\n",
            "Total average train loss: 0.0019067716547846794\n",
            "\n",
            " Test epoch: 32\n",
            "\n",
            "Total average test accuarcy: 0.8445\n",
            "Total average test loss: 0.005537957407534122\n",
            "\n",
            "경과 시간: 1368.225291967392\n",
            "Epoch: 33\n",
            "\n",
            "Total average train accuracy: 0.91416\n",
            "Total average train loss: 0.0019247457219660283\n",
            "\n",
            " Test epoch: 33\n",
            "\n",
            "Total average test accuarcy: 0.8607\n",
            "Total average test loss: 0.004462376691401005\n",
            "\n",
            "경과 시간: 1409.5448279380798\n",
            "Epoch: 34\n",
            "\n",
            "Total average train accuracy: 0.91398\n",
            "Total average train loss: 0.001910440719127655\n",
            "\n",
            " Test epoch: 34\n",
            "\n",
            "Total average test accuarcy: 0.8706\n",
            "Total average test loss: 0.004196573078632354\n",
            "\n",
            "경과 시간: 1451.0360786914825\n",
            "Epoch: 35\n",
            "\n",
            "Total average train accuracy: 0.91836\n",
            "Total average train loss: 0.0018567232973873615\n",
            "\n",
            " Test epoch: 35\n",
            "\n",
            "Total average test accuarcy: 0.8663\n",
            "Total average test loss: 0.004411980833113193\n",
            "\n",
            "경과 시간: 1492.5256989002228\n",
            "Epoch: 36\n",
            "\n",
            "Total average train accuracy: 0.92016\n",
            "Total average train loss: 0.0018006211365759373\n",
            "\n",
            " Test epoch: 36\n",
            "\n",
            "Total average test accuarcy: 0.8586\n",
            "Total average test loss: 0.00442272931933403\n",
            "\n",
            "경과 시간: 1533.9001286029816\n",
            "Epoch: 37\n",
            "\n",
            "Total average train accuracy: 0.91984\n",
            "Total average train loss: 0.0017976870474219321\n",
            "\n",
            " Test epoch: 37\n",
            "\n",
            "Total average test accuarcy: 0.8799\n",
            "Total average test loss: 0.00366119778752327\n",
            "\n",
            "경과 시간: 1575.351326227188\n",
            "Epoch: 38\n",
            "\n",
            "Total average train accuracy: 0.92044\n",
            "Total average train loss: 0.0017842752560973168\n",
            "\n",
            " Test epoch: 38\n",
            "\n",
            "Total average test accuarcy: 0.8645\n",
            "Total average test loss: 0.004278374576568603\n",
            "\n",
            "경과 시간: 1616.83038687706\n",
            "Epoch: 39\n",
            "\n",
            "Total average train accuracy: 0.92148\n",
            "Total average train loss: 0.0017551598206162454\n",
            "\n",
            " Test epoch: 39\n",
            "\n",
            "Total average test accuarcy: 0.8263\n",
            "Total average test loss: 0.005946957370638847\n",
            "\n",
            "경과 시간: 1658.4069707393646\n",
            "Epoch: 40\n",
            "\n",
            "Total average train accuracy: 0.92134\n",
            "Total average train loss: 0.0017532036308199168\n",
            "\n",
            " Test epoch: 40\n",
            "\n",
            "Total average test accuarcy: 0.8586\n",
            "Total average test loss: 0.004625098559260368\n",
            "\n",
            "경과 시간: 1700.0680756568909\n",
            "Epoch: 41\n",
            "\n",
            "Total average train accuracy: 0.92412\n",
            "Total average train loss: 0.001695746487379074\n",
            "\n",
            " Test epoch: 41\n",
            "\n",
            "Total average test accuarcy: 0.8892\n",
            "Total average test loss: 0.0033846905425190927\n",
            "\n",
            "경과 시간: 1741.5860056877136\n",
            "Epoch: 42\n",
            "\n",
            "Total average train accuracy: 0.92204\n",
            "Total average train loss: 0.0017252120089530944\n",
            "\n",
            " Test epoch: 42\n",
            "\n",
            "Total average test accuarcy: 0.8782\n",
            "Total average test loss: 0.004017867717146874\n",
            "\n",
            "경과 시간: 1783.0526423454285\n",
            "Epoch: 43\n",
            "\n",
            "Total average train accuracy: 0.92318\n",
            "Total average train loss: 0.0017144090084731578\n",
            "\n",
            " Test epoch: 43\n",
            "\n",
            "Total average test accuarcy: 0.8687\n",
            "Total average test loss: 0.004090040193498135\n",
            "\n",
            "경과 시간: 1824.4527606964111\n",
            "Epoch: 44\n",
            "\n",
            "Total average train accuracy: 0.92882\n",
            "Total average train loss: 0.0016314080196619034\n",
            "\n",
            " Test epoch: 44\n",
            "\n",
            "Total average test accuarcy: 0.8831\n",
            "Total average test loss: 0.0037070333406329153\n",
            "\n",
            "경과 시간: 1865.909280538559\n",
            "Epoch: 45\n",
            "\n",
            "Total average train accuracy: 0.92658\n",
            "Total average train loss: 0.0016195103285461664\n",
            "\n",
            " Test epoch: 45\n",
            "\n",
            "Total average test accuarcy: 0.8566\n",
            "Total average test loss: 0.0046997640639543535\n",
            "\n",
            "경과 시간: 1907.3754360675812\n",
            "Epoch: 46\n",
            "\n",
            "Total average train accuracy: 0.92772\n",
            "Total average train loss: 0.0016093589112907648\n",
            "\n",
            " Test epoch: 46\n",
            "\n",
            "Total average test accuarcy: 0.8538\n",
            "Total average test loss: 0.004960061161220074\n",
            "\n",
            "경과 시간: 1948.7743408679962\n",
            "Epoch: 47\n",
            "\n",
            "Total average train accuracy: 0.92768\n",
            "Total average train loss: 0.001629056100845337\n",
            "\n",
            " Test epoch: 47\n",
            "\n",
            "Total average test accuarcy: 0.8698\n",
            "Total average test loss: 0.004335762596130371\n",
            "\n",
            "경과 시간: 1990.3072547912598\n",
            "Epoch: 48\n",
            "\n",
            "Total average train accuracy: 0.92912\n",
            "Total average train loss: 0.0015643777419626712\n",
            "\n",
            " Test epoch: 48\n",
            "\n",
            "Total average test accuarcy: 0.8763\n",
            "Total average test loss: 0.0038202632278203966\n",
            "\n",
            "경과 시간: 2031.7680613994598\n",
            "Epoch: 49\n",
            "\n",
            "Total average train accuracy: 0.92862\n",
            "Total average train loss: 0.0015854549410939216\n",
            "\n",
            " Test epoch: 49\n",
            "\n",
            "Total average test accuarcy: 0.8709\n",
            "Total average test loss: 0.004265257409214973\n",
            "\n",
            "경과 시간: 2073.2110509872437\n",
            "Epoch: 50\n",
            "\n",
            "Total average train accuracy: 0.92866\n",
            "Total average train loss: 0.0015860397589206696\n",
            "\n",
            " Test epoch: 50\n",
            "\n",
            "Total average test accuarcy: 0.8789\n",
            "Total average test loss: 0.003779045037925243\n",
            "\n",
            "경과 시간: 2114.6782031059265\n",
            "Epoch: 51\n",
            "\n",
            "Total average train accuracy: 0.93398\n",
            "Total average train loss: 0.0014962594014406203\n",
            "\n",
            " Test epoch: 51\n",
            "\n",
            "Total average test accuarcy: 0.856\n",
            "Total average test loss: 0.005079982256889343\n",
            "\n",
            "경과 시간: 2156.230307340622\n",
            "Epoch: 52\n",
            "\n",
            "Total average train accuracy: 0.92982\n",
            "Total average train loss: 0.0015608937376737594\n",
            "\n",
            " Test epoch: 52\n",
            "\n",
            "Total average test accuarcy: 0.8715\n",
            "Total average test loss: 0.004176420739293098\n",
            "\n",
            "경과 시간: 2197.6331703662872\n",
            "Epoch: 53\n",
            "\n",
            "Total average train accuracy: 0.92842\n",
            "Total average train loss: 0.0015957072907686234\n",
            "\n",
            " Test epoch: 53\n",
            "\n",
            "Total average test accuarcy: 0.8675\n",
            "Total average test loss: 0.004442585347592831\n",
            "\n",
            "경과 시간: 2239.1271579265594\n",
            "Epoch: 54\n",
            "\n",
            "Total average train accuracy: 0.93234\n",
            "Total average train loss: 0.0014928334432840347\n",
            "\n",
            " Test epoch: 54\n",
            "\n",
            "Total average test accuarcy: 0.8781\n",
            "Total average test loss: 0.003872815801203251\n",
            "\n",
            "경과 시간: 2280.6911115646362\n",
            "Epoch: 55\n",
            "\n",
            "Total average train accuracy: 0.93178\n",
            "Total average train loss: 0.0015036251626908779\n",
            "\n",
            " Test epoch: 55\n",
            "\n",
            "Total average test accuarcy: 0.8567\n",
            "Total average test loss: 0.004897103109955787\n",
            "\n",
            "경과 시간: 2322.2131838798523\n",
            "Epoch: 56\n",
            "\n",
            "Total average train accuracy: 0.93252\n",
            "Total average train loss: 0.0015302620722353459\n",
            "\n",
            " Test epoch: 56\n",
            "\n",
            "Total average test accuarcy: 0.8692\n",
            "Total average test loss: 0.004458101633191109\n",
            "\n",
            "경과 시간: 2363.6729192733765\n",
            "Epoch: 57\n",
            "\n",
            "Total average train accuracy: 0.93594\n",
            "Total average train loss: 0.001422758751809597\n",
            "\n",
            " Test epoch: 57\n",
            "\n",
            "Total average test accuarcy: 0.8457\n",
            "Total average test loss: 0.005616275331377983\n",
            "\n",
            "경과 시간: 2405.135458946228\n",
            "Epoch: 58\n",
            "\n",
            "Total average train accuracy: 0.93412\n",
            "Total average train loss: 0.0014708866649866105\n",
            "\n",
            " Test epoch: 58\n",
            "\n",
            "Total average test accuarcy: 0.8772\n",
            "Total average test loss: 0.004158975087106228\n",
            "\n",
            "경과 시간: 2446.5634706020355\n",
            "Epoch: 59\n",
            "\n",
            "Total average train accuracy: 0.93474\n",
            "Total average train loss: 0.0014444502876698971\n",
            "\n",
            " Test epoch: 59\n",
            "\n",
            "Total average test accuarcy: 0.871\n",
            "Total average test loss: 0.0042833710566163065\n",
            "\n",
            "경과 시간: 2487.8737008571625\n",
            "Epoch: 60\n",
            "\n",
            "Total average train accuracy: 0.93284\n",
            "Total average train loss: 0.0014909221722185612\n",
            "\n",
            " Test epoch: 60\n",
            "\n",
            "Total average test accuarcy: 0.8771\n",
            "Total average test loss: 0.004191672073304654\n",
            "\n",
            "경과 시간: 2529.2319395542145\n",
            "Epoch: 61\n",
            "\n",
            "Total average train accuracy: 0.9357\n",
            "Total average train loss: 0.0014482943804562092\n",
            "\n",
            " Test epoch: 61\n",
            "\n",
            "Total average test accuarcy: 0.8587\n",
            "Total average test loss: 0.00481179833561182\n",
            "\n",
            "경과 시간: 2570.5665793418884\n",
            "Epoch: 62\n",
            "\n",
            "Total average train accuracy: 0.93482\n",
            "Total average train loss: 0.0014567363534122705\n",
            "\n",
            " Test epoch: 62\n",
            "\n",
            "Total average test accuarcy: 0.8811\n",
            "Total average test loss: 0.004099268284440041\n",
            "\n",
            "경과 시간: 2612.0721669197083\n",
            "Epoch: 63\n",
            "\n",
            "Total average train accuracy: 0.93528\n",
            "Total average train loss: 0.0014424528397619725\n",
            "\n",
            " Test epoch: 63\n",
            "\n",
            "Total average test accuarcy: 0.877\n",
            "Total average test loss: 0.004128899297118187\n",
            "\n",
            "경과 시간: 2653.3492393493652\n",
            "Epoch: 64\n",
            "\n",
            "Total average train accuracy: 0.93658\n",
            "Total average train loss: 0.0013990214263647795\n",
            "\n",
            " Test epoch: 64\n",
            "\n",
            "Total average test accuarcy: 0.8843\n",
            "Total average test loss: 0.003776307690143585\n",
            "\n",
            "경과 시간: 2694.7688388824463\n",
            "Epoch: 65\n",
            "\n",
            "Total average train accuracy: 0.93502\n",
            "Total average train loss: 0.0014301789480447768\n",
            "\n",
            " Test epoch: 65\n",
            "\n",
            "Total average test accuarcy: 0.8802\n",
            "Total average test loss: 0.003963315741717815\n",
            "\n",
            "경과 시간: 2736.0714168548584\n",
            "Epoch: 66\n",
            "\n",
            "Total average train accuracy: 0.9373\n",
            "Total average train loss: 0.001413120059221983\n",
            "\n",
            " Test epoch: 66\n",
            "\n",
            "Total average test accuarcy: 0.8817\n",
            "Total average test loss: 0.003919348149001598\n",
            "\n",
            "경과 시간: 2777.409893989563\n",
            "Epoch: 67\n",
            "\n",
            "Total average train accuracy: 0.93588\n",
            "Total average train loss: 0.0014337383565306663\n",
            "\n",
            " Test epoch: 67\n",
            "\n",
            "Total average test accuarcy: 0.8822\n",
            "Total average test loss: 0.0038173676669597625\n",
            "\n",
            "경과 시간: 2818.767446756363\n",
            "Epoch: 68\n",
            "\n",
            "Total average train accuracy: 0.93936\n",
            "Total average train loss: 0.0013608322056382894\n",
            "\n",
            " Test epoch: 68\n",
            "\n",
            "Total average test accuarcy: 0.8668\n",
            "Total average test loss: 0.004447123093903065\n",
            "\n",
            "경과 시간: 2860.1933937072754\n",
            "Epoch: 69\n",
            "\n",
            "Total average train accuracy: 0.93434\n",
            "Total average train loss: 0.00143657475233078\n",
            "\n",
            " Test epoch: 69\n",
            "\n",
            "Total average test accuarcy: 0.8803\n",
            "Total average test loss: 0.0042752426952123645\n",
            "\n",
            "경과 시간: 2901.683691740036\n",
            "Epoch: 70\n",
            "\n",
            "Total average train accuracy: 0.93796\n",
            "Total average train loss: 0.0013557493543624878\n",
            "\n",
            " Test epoch: 70\n",
            "\n",
            "Total average test accuarcy: 0.8762\n",
            "Total average test loss: 0.004185981217026711\n",
            "\n",
            "경과 시간: 2943.1420998573303\n",
            "Epoch: 71\n",
            "\n",
            "Total average train accuracy: 0.93818\n",
            "Total average train loss: 0.0013819348733127117\n",
            "\n",
            " Test epoch: 71\n",
            "\n",
            "Total average test accuarcy: 0.8803\n",
            "Total average test loss: 0.004156722499430179\n",
            "\n",
            "경과 시간: 2984.5392343997955\n",
            "Epoch: 72\n",
            "\n",
            "Total average train accuracy: 0.9405\n",
            "Total average train loss: 0.00133879015609622\n",
            "\n",
            " Test epoch: 72\n",
            "\n",
            "Total average test accuarcy: 0.8756\n",
            "Total average test loss: 0.004361137616634369\n",
            "\n",
            "경과 시간: 3025.9394130706787\n",
            "Epoch: 73\n",
            "\n",
            "Total average train accuracy: 0.94092\n",
            "Total average train loss: 0.0013283068983256817\n",
            "\n",
            " Test epoch: 73\n",
            "\n",
            "Total average test accuarcy: 0.8875\n",
            "Total average test loss: 0.0036940350234508515\n",
            "\n",
            "경과 시간: 3067.25950551033\n",
            "Epoch: 74\n",
            "\n",
            "Total average train accuracy: 0.9397\n",
            "Total average train loss: 0.0013554161639511586\n",
            "\n",
            " Test epoch: 74\n",
            "\n",
            "Total average test accuarcy: 0.8789\n",
            "Total average test loss: 0.00411826768219471\n",
            "\n",
            "경과 시간: 3108.779428243637\n",
            "Epoch: 75\n",
            "\n",
            "Total average train accuracy: 0.94006\n",
            "Total average train loss: 0.0013373818765580654\n",
            "\n",
            " Test epoch: 75\n",
            "\n",
            "Total average test accuarcy: 0.8418\n",
            "Total average test loss: 0.005823685666918755\n",
            "\n",
            "경과 시간: 3150.2032816410065\n",
            "Epoch: 76\n",
            "\n",
            "Total average train accuracy: 0.9386\n",
            "Total average train loss: 0.001357034065797925\n",
            "\n",
            " Test epoch: 76\n",
            "\n",
            "Total average test accuarcy: 0.866\n",
            "Total average test loss: 0.004631649234890938\n",
            "\n",
            "경과 시간: 3191.6305692195892\n",
            "Epoch: 77\n",
            "\n",
            "Total average train accuracy: 0.94182\n",
            "Total average train loss: 0.0012912538273632526\n",
            "\n",
            " Test epoch: 77\n",
            "\n",
            "Total average test accuarcy: 0.8596\n",
            "Total average test loss: 0.00470279711484909\n",
            "\n",
            "경과 시간: 3232.971694946289\n",
            "Epoch: 78\n",
            "\n",
            "Total average train accuracy: 0.9399\n",
            "Total average train loss: 0.0013275437093526125\n",
            "\n",
            " Test epoch: 78\n",
            "\n",
            "Total average test accuarcy: 0.8667\n",
            "Total average test loss: 0.004715406373143196\n",
            "\n",
            "경과 시간: 3274.3681647777557\n",
            "Epoch: 79\n",
            "\n",
            "Total average train accuracy: 0.94132\n",
            "Total average train loss: 0.0013137989509105683\n",
            "\n",
            " Test epoch: 79\n",
            "\n",
            "Total average test accuarcy: 0.8675\n",
            "Total average test loss: 0.004760199701786041\n",
            "\n",
            "경과 시간: 3315.6178545951843\n",
            "Epoch: 80\n",
            "\n",
            "Total average train accuracy: 0.94098\n",
            "Total average train loss: 0.0013144658398628235\n",
            "\n",
            " Test epoch: 80\n",
            "\n",
            "Total average test accuarcy: 0.8714\n",
            "Total average test loss: 0.004128741835057736\n",
            "\n",
            "경과 시간: 3357.015444278717\n",
            "Epoch: 81\n",
            "\n",
            "Total average train accuracy: 0.94296\n",
            "Total average train loss: 0.001293410598412156\n",
            "\n",
            " Test epoch: 81\n",
            "\n",
            "Total average test accuarcy: 0.8855\n",
            "Total average test loss: 0.003806309273093939\n",
            "\n",
            "경과 시간: 3398.435466527939\n",
            "Epoch: 82\n",
            "\n",
            "Total average train accuracy: 0.94096\n",
            "Total average train loss: 0.0013129156625270844\n",
            "\n",
            " Test epoch: 82\n",
            "\n",
            "Total average test accuarcy: 0.8804\n",
            "Total average test loss: 0.0039136904805898666\n",
            "\n",
            "경과 시간: 3439.662430047989\n",
            "Epoch: 83\n",
            "\n",
            "Total average train accuracy: 0.94248\n",
            "Total average train loss: 0.0012928704942017793\n",
            "\n",
            " Test epoch: 83\n",
            "\n",
            "Total average test accuarcy: 0.8481\n",
            "Total average test loss: 0.005301117068529129\n",
            "\n",
            "경과 시간: 3481.055697441101\n",
            "Epoch: 84\n",
            "\n",
            "Total average train accuracy: 0.9416\n",
            "Total average train loss: 0.0012887031391263007\n",
            "\n",
            " Test epoch: 84\n",
            "\n",
            "Total average test accuarcy: 0.8647\n",
            "Total average test loss: 0.004814332002401352\n",
            "\n",
            "경과 시간: 3522.5012152194977\n",
            "Epoch: 85\n",
            "\n",
            "Total average train accuracy: 0.94292\n",
            "Total average train loss: 0.0012840666274726391\n",
            "\n",
            " Test epoch: 85\n",
            "\n",
            "Total average test accuarcy: 0.8577\n",
            "Total average test loss: 0.004832679888606072\n",
            "\n",
            "경과 시간: 3563.847250699997\n",
            "Epoch: 86\n",
            "\n",
            "Total average train accuracy: 0.94202\n",
            "Total average train loss: 0.0013018082225322723\n",
            "\n",
            " Test epoch: 86\n",
            "\n",
            "Total average test accuarcy: 0.8832\n",
            "Total average test loss: 0.004030956029891968\n",
            "\n",
            "경과 시간: 3605.280148267746\n",
            "Epoch: 87\n",
            "\n",
            "Total average train accuracy: 0.94378\n",
            "Total average train loss: 0.0012434315149486065\n",
            "\n",
            " Test epoch: 87\n",
            "\n",
            "Total average test accuarcy: 0.8807\n",
            "Total average test loss: 0.004023168671131134\n",
            "\n",
            "경과 시간: 3646.6377577781677\n",
            "Epoch: 88\n",
            "\n",
            "Total average train accuracy: 0.93936\n",
            "Total average train loss: 0.001335905927196145\n",
            "\n",
            " Test epoch: 88\n",
            "\n",
            "Total average test accuarcy: 0.8849\n",
            "Total average test loss: 0.004110253313183785\n",
            "\n",
            "경과 시간: 3688.081306695938\n",
            "Epoch: 89\n",
            "\n",
            "Total average train accuracy: 0.94424\n",
            "Total average train loss: 0.0012216998100280762\n",
            "\n",
            " Test epoch: 89\n",
            "\n",
            "Total average test accuarcy: 0.8638\n",
            "Total average test loss: 0.004494221593439579\n",
            "\n",
            "경과 시간: 3729.5340695381165\n",
            "Epoch: 90\n",
            "\n",
            "Total average train accuracy: 0.97234\n",
            "Total average train loss: 0.000666445957235992\n",
            "\n",
            " Test epoch: 90\n",
            "\n",
            "Total average test accuarcy: 0.9201\n",
            "Total average test loss: 0.002624063927680254\n",
            "\n",
            "경과 시간: 3770.9556267261505\n",
            "Epoch: 91\n",
            "\n",
            "Total average train accuracy: 0.98246\n",
            "Total average train loss: 0.0004311445043608546\n",
            "\n",
            " Test epoch: 91\n",
            "\n",
            "Total average test accuarcy: 0.9216\n",
            "Total average test loss: 0.0026356593504548074\n",
            "\n",
            "경과 시간: 3812.3017706871033\n",
            "Epoch: 92\n",
            "\n",
            "Total average train accuracy: 0.98616\n",
            "Total average train loss: 0.00035565599417313934\n",
            "\n",
            " Test epoch: 92\n",
            "\n",
            "Total average test accuarcy: 0.9244\n",
            "Total average test loss: 0.0026647916369140147\n",
            "\n",
            "경과 시간: 3853.7663581371307\n",
            "Epoch: 93\n",
            "\n",
            "Total average train accuracy: 0.9878\n",
            "Total average train loss: 0.00031718776639550925\n",
            "\n",
            " Test epoch: 93\n",
            "\n",
            "Total average test accuarcy: 0.9245\n",
            "Total average test loss: 0.002649312686175108\n",
            "\n",
            "경과 시간: 3895.171151161194\n",
            "Epoch: 94\n",
            "\n",
            "Total average train accuracy: 0.9895\n",
            "Total average train loss: 0.00027884050089865923\n",
            "\n",
            " Test epoch: 94\n",
            "\n",
            "Total average test accuarcy: 0.9233\n",
            "Total average test loss: 0.0027035035643726587\n",
            "\n",
            "경과 시간: 3936.598142147064\n",
            "Epoch: 95\n",
            "\n",
            "Total average train accuracy: 0.99072\n",
            "Total average train loss: 0.0002475851523038\n",
            "\n",
            " Test epoch: 95\n",
            "\n",
            "Total average test accuarcy: 0.9249\n",
            "Total average test loss: 0.002733622279018164\n",
            "\n",
            "경과 시간: 3977.957993030548\n",
            "Epoch: 96\n",
            "\n",
            "Total average train accuracy: 0.9916\n",
            "Total average train loss: 0.0002320448467414826\n",
            "\n",
            " Test epoch: 96\n",
            "\n",
            "Total average test accuarcy: 0.9261\n",
            "Total average test loss: 0.0027107087410986426\n",
            "\n",
            "경과 시간: 4019.3977682590485\n",
            "Epoch: 97\n",
            "\n",
            "Total average train accuracy: 0.9917\n",
            "Total average train loss: 0.00021291670206468553\n",
            "\n",
            " Test epoch: 97\n",
            "\n",
            "Total average test accuarcy: 0.9268\n",
            "Total average test loss: 0.0028030808240175247\n",
            "\n",
            "경과 시간: 4060.7582337856293\n",
            "Epoch: 98\n",
            "\n",
            "Total average train accuracy: 0.99228\n",
            "Total average train loss: 0.00019804074457846583\n",
            "\n",
            " Test epoch: 98\n",
            "\n",
            "Total average test accuarcy: 0.9269\n",
            "Total average test loss: 0.002856356245279312\n",
            "\n",
            "경과 시간: 4102.2312433719635\n",
            "Epoch: 99\n",
            "\n",
            "Total average train accuracy: 0.99328\n",
            "Total average train loss: 0.00017899537625256927\n",
            "\n",
            " Test epoch: 99\n",
            "\n",
            "Total average test accuarcy: 0.9241\n",
            "Total average test loss: 0.002823409901559353\n",
            "\n",
            "경과 시간: 4143.60297369957\n",
            "Epoch: 100\n",
            "\n",
            "Total average train accuracy: 0.99378\n",
            "Total average train loss: 0.00016722035170998423\n",
            "\n",
            " Test epoch: 100\n",
            "\n",
            "Total average test accuarcy: 0.9272\n",
            "Total average test loss: 0.0028656084448099137\n",
            "\n",
            "경과 시간: 4184.912935018539\n",
            "Epoch: 101\n",
            "\n",
            "Total average train accuracy: 0.99396\n",
            "Total average train loss: 0.00015703766848891974\n",
            "\n",
            " Test epoch: 101\n",
            "\n",
            "Total average test accuarcy: 0.9271\n",
            "Total average test loss: 0.0028656024124473333\n",
            "\n",
            "경과 시간: 4226.399658918381\n",
            "Epoch: 102\n",
            "\n",
            "Total average train accuracy: 0.99486\n",
            "Total average train loss: 0.00015071442161221056\n",
            "\n",
            " Test epoch: 102\n",
            "\n",
            "Total average test accuarcy: 0.9265\n",
            "Total average test loss: 0.0029220770969986916\n",
            "\n",
            "경과 시간: 4267.7555809021\n",
            "Epoch: 103\n",
            "\n",
            "Total average train accuracy: 0.99472\n",
            "Total average train loss: 0.00013664615722373127\n",
            "\n",
            " Test epoch: 103\n",
            "\n",
            "Total average test accuarcy: 0.926\n",
            "Total average test loss: 0.002882046351581812\n",
            "\n",
            "경과 시간: 4309.177786111832\n",
            "Epoch: 104\n",
            "\n",
            "Total average train accuracy: 0.99556\n",
            "Total average train loss: 0.00013219877040479333\n",
            "\n",
            " Test epoch: 104\n",
            "\n",
            "Total average test accuarcy: 0.9269\n",
            "Total average test loss: 0.0029632933042943476\n",
            "\n",
            "경과 시간: 4350.648721456528\n",
            "Epoch: 105\n",
            "\n",
            "Total average train accuracy: 0.99542\n",
            "Total average train loss: 0.0001279237708542496\n",
            "\n",
            " Test epoch: 105\n",
            "\n",
            "Total average test accuarcy: 0.9273\n",
            "Total average test loss: 0.0029343389954417944\n",
            "\n",
            "경과 시간: 4392.036015033722\n",
            "Epoch: 106\n",
            "\n",
            "Total average train accuracy: 0.99614\n",
            "Total average train loss: 0.00011501032534055412\n",
            "\n",
            " Test epoch: 106\n",
            "\n",
            "Total average test accuarcy: 0.9277\n",
            "Total average test loss: 0.0029972595762461423\n",
            "\n",
            "경과 시간: 4433.441295623779\n",
            "Epoch: 107\n",
            "\n",
            "Total average train accuracy: 0.99594\n",
            "Total average train loss: 0.00011748769961297512\n",
            "\n",
            " Test epoch: 107\n",
            "\n",
            "Total average test accuarcy: 0.9279\n",
            "Total average test loss: 0.0029675236955285073\n",
            "\n",
            "경과 시간: 4474.8938591480255\n",
            "Epoch: 108\n",
            "\n",
            "Total average train accuracy: 0.99658\n",
            "Total average train loss: 0.00010039318439317868\n",
            "\n",
            " Test epoch: 108\n",
            "\n",
            "Total average test accuarcy: 0.9263\n",
            "Total average test loss: 0.003040862748026848\n",
            "\n",
            "경과 시간: 4516.206744432449\n",
            "Epoch: 109\n",
            "\n",
            "Total average train accuracy: 0.99676\n",
            "Total average train loss: 0.00010208514457102865\n",
            "\n",
            " Test epoch: 109\n",
            "\n",
            "Total average test accuarcy: 0.9285\n",
            "Total average test loss: 0.0029843858808279037\n",
            "\n",
            "경과 시간: 4557.544281721115\n",
            "Epoch: 110\n",
            "\n",
            "Total average train accuracy: 0.99682\n",
            "Total average train loss: 9.792877695988864e-05\n",
            "\n",
            " Test epoch: 110\n",
            "\n",
            "Total average test accuarcy: 0.9264\n",
            "Total average test loss: 0.003032926754653454\n",
            "\n",
            "경과 시간: 4598.905354976654\n",
            "Epoch: 111\n",
            "\n",
            "Total average train accuracy: 0.9967\n",
            "Total average train loss: 9.763549438212067e-05\n",
            "\n",
            " Test epoch: 111\n",
            "\n",
            "Total average test accuarcy: 0.9276\n",
            "Total average test loss: 0.0031067644756287336\n",
            "\n",
            "경과 시간: 4640.339827537537\n",
            "Epoch: 112\n",
            "\n",
            "Total average train accuracy: 0.99712\n",
            "Total average train loss: 8.777699242578819e-05\n",
            "\n",
            " Test epoch: 112\n",
            "\n",
            "Total average test accuarcy: 0.927\n",
            "Total average test loss: 0.0031206834822893144\n",
            "\n",
            "경과 시간: 4681.723308801651\n",
            "Epoch: 113\n",
            "\n",
            "Total average train accuracy: 0.99716\n",
            "Total average train loss: 8.80494020995684e-05\n",
            "\n",
            " Test epoch: 113\n",
            "\n",
            "Total average test accuarcy: 0.9256\n",
            "Total average test loss: 0.0031462844461202623\n",
            "\n",
            "경과 시간: 4723.153349876404\n",
            "Epoch: 114\n",
            "\n",
            "Total average train accuracy: 0.99762\n",
            "Total average train loss: 8.014830290572717e-05\n",
            "\n",
            " Test epoch: 114\n",
            "\n",
            "Total average test accuarcy: 0.9263\n",
            "Total average test loss: 0.0031778033301234246\n",
            "\n",
            "경과 시간: 4764.606074333191\n",
            "Epoch: 115\n",
            "\n",
            "Total average train accuracy: 0.99772\n",
            "Total average train loss: 7.4751028760802e-05\n",
            "\n",
            " Test epoch: 115\n",
            "\n",
            "Total average test accuarcy: 0.9261\n",
            "Total average test loss: 0.003172075568139553\n",
            "\n",
            "경과 시간: 4806.066615819931\n",
            "Epoch: 116\n",
            "\n",
            "Total average train accuracy: 0.99752\n",
            "Total average train loss: 7.681301771663129e-05\n",
            "\n",
            " Test epoch: 116\n",
            "\n",
            "Total average test accuarcy: 0.9282\n",
            "Total average test loss: 0.003143093217164278\n",
            "\n",
            "경과 시간: 4847.521881341934\n",
            "Epoch: 117\n",
            "\n",
            "Total average train accuracy: 0.9978\n",
            "Total average train loss: 7.423417045734822e-05\n",
            "\n",
            " Test epoch: 117\n",
            "\n",
            "Total average test accuarcy: 0.9278\n",
            "Total average test loss: 0.0031743235647678377\n",
            "\n",
            "경과 시간: 4889.042895793915\n",
            "Epoch: 118\n",
            "\n",
            "Total average train accuracy: 0.99766\n",
            "Total average train loss: 7.574222587281838e-05\n",
            "\n",
            " Test epoch: 118\n",
            "\n",
            "Total average test accuarcy: 0.9288\n",
            "Total average test loss: 0.0031396340910345316\n",
            "\n",
            "경과 시간: 4930.44651222229\n",
            "Epoch: 119\n",
            "\n",
            "Total average train accuracy: 0.99794\n",
            "Total average train loss: 6.85879025887698e-05\n",
            "\n",
            " Test epoch: 119\n",
            "\n",
            "Total average test accuarcy: 0.9268\n",
            "Total average test loss: 0.003142380528897047\n",
            "\n",
            "경과 시간: 4972.010073423386\n",
            "Epoch: 120\n",
            "\n",
            "Total average train accuracy: 0.9976\n",
            "Total average train loss: 7.406244791229255e-05\n",
            "\n",
            " Test epoch: 120\n",
            "\n",
            "Total average test accuarcy: 0.9269\n",
            "Total average test loss: 0.0032248645797371862\n",
            "\n",
            "경과 시간: 5013.479051828384\n",
            "Epoch: 121\n",
            "\n",
            "Total average train accuracy: 0.99784\n",
            "Total average train loss: 6.953476171125659e-05\n",
            "\n",
            " Test epoch: 121\n",
            "\n",
            "Total average test accuarcy: 0.9274\n",
            "Total average test loss: 0.0032003381565213203\n",
            "\n",
            "경과 시간: 5054.995658636093\n",
            "Epoch: 122\n",
            "\n",
            "Total average train accuracy: 0.99826\n",
            "Total average train loss: 6.33901651832275e-05\n",
            "\n",
            " Test epoch: 122\n",
            "\n",
            "Total average test accuarcy: 0.9271\n",
            "Total average test loss: 0.003202008043974638\n",
            "\n",
            "경과 시간: 5096.42956495285\n",
            "Epoch: 123\n",
            "\n",
            "Total average train accuracy: 0.99812\n",
            "Total average train loss: 6.259564564446918e-05\n",
            "\n",
            " Test epoch: 123\n",
            "\n",
            "Total average test accuarcy: 0.9263\n",
            "Total average test loss: 0.003279963839799166\n",
            "\n",
            "경과 시간: 5137.991227388382\n",
            "Epoch: 124\n",
            "\n",
            "Total average train accuracy: 0.99854\n",
            "Total average train loss: 5.704748284537345e-05\n",
            "\n",
            " Test epoch: 124\n",
            "\n",
            "Total average test accuarcy: 0.9285\n",
            "Total average test loss: 0.0032141992654651404\n",
            "\n",
            "경과 시간: 5179.582559108734\n",
            "Epoch: 125\n",
            "\n",
            "Total average train accuracy: 0.99868\n",
            "Total average train loss: 5.276366798090749e-05\n",
            "\n",
            " Test epoch: 125\n",
            "\n",
            "Total average test accuarcy: 0.9288\n",
            "Total average test loss: 0.0031559813529253005\n",
            "\n",
            "경과 시간: 5221.173436164856\n",
            "Epoch: 126\n",
            "\n",
            "Total average train accuracy: 0.99852\n",
            "Total average train loss: 6.0242661925731224e-05\n",
            "\n",
            " Test epoch: 126\n",
            "\n",
            "Total average test accuarcy: 0.9293\n",
            "Total average test loss: 0.0032898894023150205\n",
            "\n",
            "경과 시간: 5262.663938522339\n",
            "Epoch: 127\n",
            "\n",
            "Total average train accuracy: 0.9984\n",
            "Total average train loss: 5.4573706715600565e-05\n",
            "\n",
            " Test epoch: 127\n",
            "\n",
            "Total average test accuarcy: 0.9284\n",
            "Total average test loss: 0.0033348921313881875\n",
            "\n",
            "경과 시간: 5304.259242534637\n",
            "Epoch: 128\n",
            "\n",
            "Total average train accuracy: 0.99806\n",
            "Total average train loss: 5.937225908681285e-05\n",
            "\n",
            " Test epoch: 128\n",
            "\n",
            "Total average test accuarcy: 0.9282\n",
            "Total average test loss: 0.0032956190321594475\n",
            "\n",
            "경과 시간: 5345.87624168396\n",
            "Epoch: 129\n",
            "\n",
            "Total average train accuracy: 0.9985\n",
            "Total average train loss: 5.1874956820975056e-05\n",
            "\n",
            " Test epoch: 129\n",
            "\n",
            "Total average test accuarcy: 0.9281\n",
            "Total average test loss: 0.003301667535677552\n",
            "\n",
            "경과 시간: 5387.466046094894\n",
            "Epoch: 130\n",
            "\n",
            "Total average train accuracy: 0.9982\n",
            "Total average train loss: 5.716465635690838e-05\n",
            "\n",
            " Test epoch: 130\n",
            "\n",
            "Total average test accuarcy: 0.927\n",
            "Total average test loss: 0.003271734653413296\n",
            "\n",
            "경과 시간: 5429.078289747238\n",
            "Epoch: 131\n",
            "\n",
            "Total average train accuracy: 0.99884\n",
            "Total average train loss: 4.7393306456506254e-05\n",
            "\n",
            " Test epoch: 131\n",
            "\n",
            "Total average test accuarcy: 0.928\n",
            "Total average test loss: 0.003320076599344611\n",
            "\n",
            "경과 시간: 5470.68970990181\n",
            "Epoch: 132\n",
            "\n",
            "Total average train accuracy: 0.99834\n",
            "Total average train loss: 5.3729251348413525e-05\n",
            "\n",
            " Test epoch: 132\n",
            "\n",
            "Total average test accuarcy: 0.9288\n",
            "Total average test loss: 0.00330678703263402\n",
            "\n",
            "경과 시간: 5512.304691076279\n",
            "Epoch: 133\n",
            "\n",
            "Total average train accuracy: 0.99822\n",
            "Total average train loss: 5.525287248659879e-05\n",
            "\n",
            " Test epoch: 133\n",
            "\n",
            "Total average test accuarcy: 0.9281\n",
            "Total average test loss: 0.003341571022197604\n",
            "\n",
            "경과 시간: 5553.8757445812225\n",
            "Epoch: 134\n",
            "\n",
            "Total average train accuracy: 0.9983\n",
            "Total average train loss: 5.294468608335592e-05\n",
            "\n",
            " Test epoch: 134\n",
            "\n",
            "Total average test accuarcy: 0.9272\n",
            "Total average test loss: 0.0033347007419914005\n",
            "\n",
            "경과 시간: 5595.528537511826\n",
            "Epoch: 135\n",
            "\n",
            "Total average train accuracy: 0.99892\n",
            "Total average train loss: 4.278914038673974e-05\n",
            "\n",
            " Test epoch: 135\n",
            "\n",
            "Total average test accuarcy: 0.9288\n",
            "Total average test loss: 0.003339430257678032\n",
            "\n",
            "경과 시간: 5637.234797716141\n",
            "Epoch: 136\n",
            "\n",
            "Total average train accuracy: 0.99878\n",
            "Total average train loss: 4.517870613839477e-05\n",
            "\n",
            " Test epoch: 136\n",
            "\n",
            "Total average test accuarcy: 0.9296\n",
            "Total average test loss: 0.003300995120406151\n",
            "\n",
            "경과 시간: 5678.862157344818\n",
            "Epoch: 137\n",
            "\n",
            "Total average train accuracy: 0.99872\n",
            "Total average train loss: 4.522171068470925e-05\n",
            "\n",
            " Test epoch: 137\n",
            "\n",
            "Total average test accuarcy: 0.9301\n",
            "Total average test loss: 0.0032437588695436717\n",
            "\n",
            "경과 시간: 5720.408630371094\n",
            "Epoch: 138\n",
            "\n",
            "Total average train accuracy: 0.99884\n",
            "Total average train loss: 4.153852865681983e-05\n",
            "\n",
            " Test epoch: 138\n",
            "\n",
            "Total average test accuarcy: 0.9303\n",
            "Total average test loss: 0.0032554231718182566\n",
            "\n",
            "경과 시간: 5761.956914424896\n",
            "Epoch: 139\n",
            "\n",
            "Total average train accuracy: 0.999\n",
            "Total average train loss: 3.968860246648546e-05\n",
            "\n",
            " Test epoch: 139\n",
            "\n",
            "Total average test accuarcy: 0.9293\n",
            "Total average test loss: 0.0032804666854441165\n",
            "\n",
            "경과 시간: 5803.514198541641\n",
            "Epoch: 140\n",
            "\n",
            "Total average train accuracy: 0.99924\n",
            "Total average train loss: 3.6129653918324036e-05\n",
            "\n",
            " Test epoch: 140\n",
            "\n",
            "Total average test accuarcy: 0.93\n",
            "Total average test loss: 0.003250134126096964\n",
            "\n",
            "경과 시간: 5845.043362379074\n",
            "Epoch: 141\n",
            "\n",
            "Total average train accuracy: 0.99904\n",
            "Total average train loss: 3.6859940403373914e-05\n",
            "\n",
            " Test epoch: 141\n",
            "\n",
            "Total average test accuarcy: 0.9307\n",
            "Total average test loss: 0.0032645259715616704\n",
            "\n",
            "경과 시간: 5886.559015274048\n",
            "Epoch: 142\n",
            "\n",
            "Total average train accuracy: 0.9991\n",
            "Total average train loss: 3.798698423372116e-05\n",
            "\n",
            " Test epoch: 142\n",
            "\n",
            "Total average test accuarcy: 0.9297\n",
            "Total average test loss: 0.003237050583958626\n",
            "\n",
            "경과 시간: 5928.1403295993805\n",
            "Epoch: 143\n",
            "\n",
            "Total average train accuracy: 0.99894\n",
            "Total average train loss: 3.76256789121544e-05\n",
            "\n",
            " Test epoch: 143\n",
            "\n",
            "Total average test accuarcy: 0.9305\n",
            "Total average test loss: 0.003278226439282298\n",
            "\n",
            "경과 시간: 5969.6477427482605\n",
            "Epoch: 144\n",
            "\n",
            "Total average train accuracy: 0.99908\n",
            "Total average train loss: 3.5958653726847845e-05\n",
            "\n",
            " Test epoch: 144\n",
            "\n",
            "Total average test accuarcy: 0.9309\n",
            "Total average test loss: 0.0032515192605555056\n",
            "\n",
            "경과 시간: 6011.175144195557\n",
            "Epoch: 145\n",
            "\n",
            "Total average train accuracy: 0.99912\n",
            "Total average train loss: 3.5028912596171725e-05\n",
            "\n",
            " Test epoch: 145\n",
            "\n",
            "Total average test accuarcy: 0.9305\n",
            "Total average test loss: 0.0032568198163062333\n",
            "\n",
            "경과 시간: 6052.7102036476135\n",
            "Epoch: 146\n",
            "\n",
            "Total average train accuracy: 0.99928\n",
            "Total average train loss: 3.227403617347591e-05\n",
            "\n",
            " Test epoch: 146\n",
            "\n",
            "Total average test accuarcy: 0.9306\n",
            "Total average test loss: 0.003259423715993762\n",
            "\n",
            "경과 시간: 6094.272154092789\n",
            "Epoch: 147\n",
            "\n",
            "Total average train accuracy: 0.99928\n",
            "Total average train loss: 3.258619457483292e-05\n",
            "\n",
            " Test epoch: 147\n",
            "\n",
            "Total average test accuarcy: 0.9304\n",
            "Total average test loss: 0.0032644365448504685\n",
            "\n",
            "경과 시간: 6135.845331430435\n",
            "Epoch: 148\n",
            "\n",
            "Total average train accuracy: 0.99908\n",
            "Total average train loss: 3.494098846160341e-05\n",
            "\n",
            " Test epoch: 148\n",
            "\n",
            "Total average test accuarcy: 0.9307\n",
            "Total average test loss: 0.0032690656907856464\n",
            "\n",
            "경과 시간: 6177.461730480194\n",
            "Epoch: 149\n",
            "\n",
            "Total average train accuracy: 0.99948\n",
            "Total average train loss: 3.227835988276638e-05\n",
            "\n",
            " Test epoch: 149\n",
            "\n",
            "Total average test accuarcy: 0.9303\n",
            "Total average test loss: 0.0032484319169074297\n",
            "\n",
            "경과 시간: 6219.021100521088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY--I0Xj8uBa"
      },
      "source": [
        "#### Plain-20 Net Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMNFB4m32-Wu"
      },
      "source": [
        "# Plain Block class\n",
        "# Residual Block에서 shortcut connection 부분만 제외시키고 나머지는 동일하게 한다.\n",
        "class Block(nn.Module):\n",
        "  #in_plane : input의 dimension, planes: output의 dimension)\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    #Block 호출 시 nn.Module 호출\n",
        "    super(Block, self).__init__()\n",
        "    # Convolution layer 정의\n",
        "    # Plain block은 두 개의 3x3 Conv layer로 구성되어 있으며 Conv => BN => Activation 과정을 거친다\n",
        "    # 논문에 따라 bias는 False로 지정\n",
        "    \n",
        "    # 첫 번째 Convolution Layer in Plain Block\n",
        "    # filter 수가 2배씩 증가하므로 너비 x 높이를 2배로 줄이고자 할때는 pooling이 아닌 stride를 2로 조정\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    # Conv layer를 거쳐 나온 output을 Batch Normalize\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    # 두 번째 Convolution Layer in Plain Block\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    \n",
        "  #순전파 순서 정의\n",
        "  def forward(self, x):\n",
        "    # Conv => BN => Activation 순으로 진행\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out = F.relu(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXmM3mKI3i_l"
      },
      "source": [
        "# PlainNet 전체 Network 구성하는 Class\n",
        "class PlainNet(nn.Module):\n",
        "  #CIFAR-10 데이터셋의 클래스 10개에 맞추어 Parameter 조정\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(PlainNet, self).__init__()\n",
        "    self.in_planes = 16\n",
        "\n",
        "    # 3의 input dimension(RGB)를 받아 16개 feature map 생성\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "    self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    #num_blocks의 갯수 만큼 strides 리스트에 넣는다 => list의 length만큼 layer 내에 block 만든다\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes # 다음 layer로 넘어갈때 채널 수 맞춰주기\n",
        "    # *args: 가변 갯수의 인자를 함수에 집어넣어 줌\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  #순전파 방식\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out) #out: [batch_size, 64, 8,8]\n",
        "    #1x1로 바꿔주기 위해서 8x8 maxpolling\n",
        "    out = F.avg_pool2d(out, 8)\n",
        "    # view: pytorch에서 reshape과 같은 역할을 함\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0qrp2v14CNc"
      },
      "source": [
        "# Plain Net-20 함수 정의\n",
        "def PlainNet20():\n",
        "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
        "  return PlainNet(Block, [3,3,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGLZeJuv4RAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "cc630bff-6c1d-49d8-9f64-c6564b8174f1"
      },
      "source": [
        "#신경망 선언\n",
        "net = PlainNet20()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'plain20_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/Plain20-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    #100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/Plain20-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint' + file_name)\n",
        "    print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 1]\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "16\n",
            "[2, 1, 1]\n",
            "16\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "32\n",
            "[2, 1, 1]\n",
            "32\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n",
            "64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xLIgk9KS5O-l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "78e5f3b9-04db-4e33-9810-024d4d671770"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
        "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
        "  if epoch >=90:\n",
        "    lr /= 10\n",
        "  if epoch >= 137:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0,150):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  print('\\n경과 시간:', time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "\n",
            "Total average train accuracy: 0.29128\n",
            "Total average train loss: 0.014599162085056305\n",
            "\n",
            " Test epoch: 0\n",
            "\n",
            "Total average test accuarcy: 0.3126\n",
            "Total average test loss: 0.0204616308093071\n",
            "\n",
            "경과 시간: 15.953054428100586\n",
            "Epoch: 1\n",
            "\n",
            "Total average train accuracy: 0.43698\n",
            "Total average train loss: 0.011952872285842896\n",
            "\n",
            " Test epoch: 1\n",
            "\n",
            "Total average test accuarcy: 0.4074\n",
            "Total average test loss: 0.01750745793581009\n",
            "\n",
            "경과 시간: 31.879066228866577\n",
            "Epoch: 2\n",
            "\n",
            "Total average train accuracy: 0.547\n",
            "Total average train loss: 0.009807555528879165\n",
            "\n",
            " Test epoch: 2\n",
            "\n",
            "Total average test accuarcy: 0.5014\n",
            "Total average test loss: 0.014287088298797608\n",
            "\n",
            "경과 시간: 47.821603536605835\n",
            "Epoch: 3\n",
            "\n",
            "Total average train accuracy: 0.63224\n",
            "Total average train loss: 0.008154440177679061\n",
            "\n",
            " Test epoch: 3\n",
            "\n",
            "Total average test accuarcy: 0.6196\n",
            "Total average test loss: 0.010738597893714905\n",
            "\n",
            "경과 시간: 63.71850895881653\n",
            "Epoch: 4\n",
            "\n",
            "Total average train accuracy: 0.67786\n",
            "Total average train loss: 0.007187452507019043\n",
            "\n",
            " Test epoch: 4\n",
            "\n",
            "Total average test accuarcy: 0.5582\n",
            "Total average test loss: 0.013265909343957901\n",
            "\n",
            "경과 시간: 79.70639729499817\n",
            "Epoch: 5\n",
            "\n",
            "Total average train accuracy: 0.71022\n",
            "Total average train loss: 0.006470948293209076\n",
            "\n",
            " Test epoch: 5\n",
            "\n",
            "Total average test accuarcy: 0.662\n",
            "Total average test loss: 0.010128256642818452\n",
            "\n",
            "경과 시간: 95.6445631980896\n",
            "Epoch: 6\n",
            "\n",
            "Total average train accuracy: 0.73774\n",
            "Total average train loss: 0.005943937330245972\n",
            "\n",
            " Test epoch: 6\n",
            "\n",
            "Total average test accuarcy: 0.6503\n",
            "Total average test loss: 0.011310603737831116\n",
            "\n",
            "경과 시간: 111.70896196365356\n",
            "Epoch: 7\n",
            "\n",
            "Total average train accuracy: 0.75354\n",
            "Total average train loss: 0.005600000123977661\n",
            "\n",
            " Test epoch: 7\n",
            "\n",
            "Total average test accuarcy: 0.7208\n",
            "Total average test loss: 0.008243122160434723\n",
            "\n",
            "경과 시간: 127.7104856967926\n",
            "Epoch: 8\n",
            "\n",
            "Total average train accuracy: 0.76636\n",
            "Total average train loss: 0.00523461410522461\n",
            "\n",
            " Test epoch: 8\n",
            "\n",
            "Total average test accuarcy: 0.707\n",
            "Total average test loss: 0.009189747804403304\n",
            "\n",
            "경과 시간: 143.73504495620728\n",
            "Epoch: 9\n",
            "\n",
            "Total average train accuracy: 0.77988\n",
            "Total average train loss: 0.004989496945738792\n",
            "\n",
            " Test epoch: 9\n",
            "\n",
            "Total average test accuarcy: 0.7773\n",
            "Total average test loss: 0.006478599134087563\n",
            "\n",
            "경과 시간: 159.65987181663513\n",
            "Epoch: 10\n",
            "\n",
            "Total average train accuracy: 0.78916\n",
            "Total average train loss: 0.004764208285808563\n",
            "\n",
            " Test epoch: 10\n",
            "\n",
            "Total average test accuarcy: 0.6545\n",
            "Total average test loss: 0.01228933990597725\n",
            "\n",
            "경과 시간: 175.62980318069458\n",
            "Epoch: 11\n",
            "\n",
            "Total average train accuracy: 0.79908\n",
            "Total average train loss: 0.0045765696322917935\n",
            "\n",
            " Test epoch: 11\n",
            "\n",
            "Total average test accuarcy: 0.7166\n",
            "Total average test loss: 0.008436118400096893\n",
            "\n",
            "경과 시간: 191.664235830307\n",
            "Epoch: 12\n",
            "\n",
            "Total average train accuracy: 0.80338\n",
            "Total average train loss: 0.004423716801702976\n",
            "\n",
            " Test epoch: 12\n",
            "\n",
            "Total average test accuarcy: 0.7493\n",
            "Total average test loss: 0.007828285938501357\n",
            "\n",
            "경과 시간: 207.61685061454773\n",
            "Epoch: 13\n",
            "\n",
            "Total average train accuracy: 0.8101\n",
            "Total average train loss: 0.004302553147673607\n",
            "\n",
            " Test epoch: 13\n",
            "\n",
            "Total average test accuarcy: 0.7371\n",
            "Total average test loss: 0.007950030156970024\n",
            "\n",
            "경과 시간: 223.63520669937134\n",
            "Epoch: 14\n",
            "\n",
            "Total average train accuracy: 0.8166\n",
            "Total average train loss: 0.004129343619346619\n",
            "\n",
            " Test epoch: 14\n",
            "\n",
            "Total average test accuarcy: 0.7137\n",
            "Total average test loss: 0.009033874154090881\n",
            "\n",
            "경과 시간: 239.60422587394714\n",
            "Epoch: 15\n",
            "\n",
            "Total average train accuracy: 0.81976\n",
            "Total average train loss: 0.004059070283770561\n",
            "\n",
            " Test epoch: 15\n",
            "\n",
            "Total average test accuarcy: 0.726\n",
            "Total average test loss: 0.009110521942377091\n",
            "\n",
            "경과 시간: 255.5863916873932\n",
            "Epoch: 16\n",
            "\n",
            "Total average train accuracy: 0.82524\n",
            "Total average train loss: 0.003960900908112526\n",
            "\n",
            " Test epoch: 16\n",
            "\n",
            "Total average test accuarcy: 0.7715\n",
            "Total average test loss: 0.007197985675930977\n",
            "\n",
            "경과 시간: 271.61271595954895\n",
            "Epoch: 17\n",
            "\n",
            "Total average train accuracy: 0.83032\n",
            "Total average train loss: 0.003856182372570038\n",
            "\n",
            " Test epoch: 17\n",
            "\n",
            "Total average test accuarcy: 0.8069\n",
            "Total average test loss: 0.0055942824184894565\n",
            "\n",
            "경과 시간: 287.6588771343231\n",
            "Epoch: 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ab8334aec2ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n경과 시간:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-d8eae67da794>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# loss back propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-ebc3a0acee0e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#out: [batch_size, 64, 8,8]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#1x1로 바꿔주기 위해서 8x8 maxpolling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c39d3585bff9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Conv => BN => Activation 순으로 진행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2150\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2151\u001b[0m     )\n\u001b[1;32m   2152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSdRq1RDl3i"
      },
      "source": [
        "### Plain-56 NET Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00oCUVLVDlfg"
      },
      "source": [
        "# PlainNet-56 함수 정의\n",
        "def PlainNet56():\n",
        "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
        "  return PlainNet(Block, [9,9,9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCZvFaXWDkzV"
      },
      "source": [
        "#신경망 선언\n",
        "net = PlainNet56()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'plain56_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/Plain56-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    #100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/Plain56-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint' + file_name)\n",
        "    print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}