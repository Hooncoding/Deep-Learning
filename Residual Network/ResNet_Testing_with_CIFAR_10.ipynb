{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_Testing_with_CIFAR-10",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OANaZjxisFr"
      },
      "source": [
        "## Residual Network Practice with Pytorch\n",
        "\n",
        "이 note에서는 ResNet 논문을 기반으로 ResNet의 성능을 Testing 해본다.\n",
        "\n",
        "\n",
        "### 사용할 학습 데이터\n",
        "- ImageNet\n",
        "- CIFAR-10\n",
        "\n",
        "### 사용할 모델\n",
        "- ImageNet dataset\n",
        "  - Pre-trained Model\n",
        "- CIFAR-10\n",
        "  - Plain Net - 20, 56\n",
        "  - ResNet - 20, 56"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPkvv9vTm7nL"
      },
      "source": [
        "# Library Import\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxWRzJN8pEKy"
      },
      "source": [
        "# GPU setting\n",
        "use_cuda = True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRNSZsGvpP64"
      },
      "source": [
        "ImageNet 클래스(1000개) 정보 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzOrTWhVpW_5"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import json\n",
        "\n",
        "# ImageNet dataset class값 가져오기\n",
        "imagenet_json, _ = urlretrieve('https://www.anishathalye.com/media/2017/07/25/imagenet.json')\n",
        "with open(imagenet_json) as f:\n",
        "  imagenet_labels = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX1roEgepuuf"
      },
      "source": [
        "# testing code\n",
        "print(imagenet_labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv5BBxfIqN04"
      },
      "source": [
        "Data 전처리는 논문과 같이 처리한다.  \n",
        "  \n",
        "\"The image is resized with its shorter side randomly sampled in [256, 480] for scale augmentation [41].\n",
        "A 224×224 crop is randomly sampled from an image or its\n",
        "horizontal flip\"(page 4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWjOVdukqzgR"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "                                 transforms.Resize(256),\n",
        "                                 transforms.CenterCrop(224),\n",
        "                                 transforms.ToTensor() #PIL 혹은 np.array를 tensor로 바꾸어준다.\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBZbexDxreZ3"
      },
      "source": [
        "# image plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVhyuU6CrlqG"
      },
      "source": [
        "# 경로로 부터 이미지를 받아와 preprocess 단계를 거쳐 tensor로 변환\n",
        "def image_loader(path):\n",
        "  image = PIL.Image.open(path)\n",
        "  # 전처리를 거친 후 network 입력단에 들어갈 배치 목적의 차원 추가 (가로, 세로, 채널) => (배치, 가로, 세로, 채널)\n",
        "  image = preprocess(image).unsqueeze(0)\n",
        "  # 처리된 image를 device에 등록\n",
        "  return image.to(device, torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32fFy_SksGPB"
      },
      "source": [
        "# Preprocessing testing\n",
        "\n",
        "image_path = './drive/MyDrive/Dev-dl/cat.jpg'\n",
        "image = image_loader(image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ35mkVDs_WW"
      },
      "source": [
        "#Test Image Plotting\n",
        "\n",
        "def imshow(tensor):\n",
        "  #matplotlib는 GPU를 지원하지 않으므로 CPU로 이동\n",
        "  image = tensor.cpu().clone()\n",
        "  #맨 앞 차원(배치) 제거\n",
        "  image = image.squeeze(0)\n",
        "  # PIL 객체로 변경\n",
        "  image = transforms.ToPILImage()(image)\n",
        "  plt.imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4VIk2UNtcx2"
      },
      "source": [
        "plt.figure()\n",
        "imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmZI31D0tiHm"
      },
      "source": [
        "###Pretrained 모델 불러오기\n",
        "\n",
        "- ImageNet dataset은 데이터 크기가 매우 크기 때문에 새로 학습을 시키는 것은 시간 상 생략\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z71LX34t9P2",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "# input data normalization class\n",
        "class Normalize(nn.Module):\n",
        "  def __init__(self, mean, std):\n",
        "    super(Normalize, self).__init__()\n",
        "    #Normalize method => Standardization\n",
        "    self.register_buffer('mean', torch.Tensor(mean))\n",
        "    self.register_buffer('std', torch.Tensor(std))\n",
        "\n",
        "  def forward(self, input):\n",
        "    mean = self.mean.reshape(1, 3, 1, 1)\n",
        "    std = self.std.reshape(1, 3, 1, 1)\n",
        "    return (input - mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxwUoDonvcNW"
      },
      "source": [
        "# 모델 실행시 input에 대한 Normalize 수행 후 실행\n",
        "model = nn.Sequential(\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
        ").to(device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "993ihnsdv99W"
      },
      "source": [
        "# Test image를 넣어 Prediction 수행\n",
        "outputs = model(image)\n",
        "percentages = torch.nn.functional.softmax(outputs, dim=1)[0] * 100\n",
        "# 가장 높은 값을 가지는 5개의 인덱스와 그 확률을 출력\n",
        "print(\"결과\")\n",
        "for i in outputs[0].topk(5)[1]:\n",
        "  print(f'인덱스: {i.item()}, 클래스명: {imagenet_labels[i]}, 확률: {round(percentages[i].item(),4)}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSFm9pmWwzZG"
      },
      "source": [
        "## 다음으로 CIFAR-10 DATASET을 대상으로 실제 RESNET 모델을 Build 해보고 학습해본다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPWrtl9MzuZW"
      },
      "source": [
        "# Library import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TkegeZTz6Kc"
      },
      "source": [
        "# Residual Block class\n",
        "class ResidualBlock(nn.Module):\n",
        "  #in_plane : input의 dimension, planes: output의 dimension)\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    #ResidualBlock 호출 시 nn.Module 호출\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    # Convolution layer 정의\n",
        "    # Residual block은 두 개의 3x3 Conv layer로 구성되어 있으며 Conv => BN => Activation 과정을 거친다\n",
        "    # 논문에 따라 bias는 False로 지정\n",
        "    \n",
        "    # 첫 번째 Convolution Layer in Residual block\n",
        "    # filter 수가 2배씩 증가하므로 너비 x 높이를 2배로 줄이고자 할때는 pooling이 아닌 stride를 2로 조정\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    # Conv layer를 거쳐 나온 output을 Batch Normalize\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    # 두 번째 Convolution Layer in Residual block\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    \n",
        "    # Stride = 1일 경우 input dimension == output dimension => identity mapping 수행\n",
        "    # argument가 없이 nn.Sequential()을 호출할 경우 identity mapping 수행\n",
        "    self.shortcut = nn.Sequential()\n",
        "    # Stride = 2일 경우 input dimension != output dimension => stride=2, 1x1 합성곱망을 거치도록 함\n",
        "    if stride != 1:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          #여기서 planes가 아닌 in_planes가 들어가는 이유\n",
        "            #shortcut은 input에 대해 identity mapping을 하는 것이므로 input의 dimension을 받는다\n",
        "          nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "          nn.BatchNorm2d(planes)\n",
        "      )\n",
        "  #순전파 순서 정의\n",
        "  def forward(self, x):\n",
        "    # Conv => BN => Activation 순으로 진행\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    #shortcut connection\n",
        "    out += self.shortcut(x)\n",
        "    out = F.relu(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v7m_4YONwVr"
      },
      "source": [
        "### 레이어 구성 목표\n",
        "\n",
        "1. The first layer is 3×3 convolutions.\n",
        "2. Then we use a stack of 6n layers with 3×3 convolutions on the feature maps of sizes {32, 16, 8} respectively,\n",
        "with 2n layers for each feature map size. The numbers of\n",
        "filters are {16, 32, 64} respectively.\n",
        "3. The network ends\n",
        "with a global average pooling, a 10-way fully-connected\n",
        "layer, and softmax. \n",
        "4. Plain Net - 20, 56 / ResNet - 20, 56"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZBtarzQ6MK1"
      },
      "source": [
        "# ResNet 전체 Network 구성하는 Class\n",
        "class ResNet(nn.Module):\n",
        "  #CIFAR-10 데이터셋의 클래스 10개에 맞추어 Parameter 조정\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_planes = 16\n",
        "\n",
        "    # ImageNet 처리 시 가장 앞 단에 layer 7x7, maxPooling 층을 두었으나\n",
        "    # 3의 input dimension(RGB)를 받아 64개 feature map 생성\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "    self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    #num_blocks의 갯수 만큼 strides 리스트에 넣는다 => list의 length만큼 layer 내에 block 만든다\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes # 다음 layer로 넘어갈때 채널 수 맞춰주기\n",
        "    # *args: 가변 갯수의 인자를 함수에 집어넣어 줌\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  #순전파 방식\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out) #out: [batch_size, 64, 8,8]\n",
        "    #1x1로 바꿔주기 위해서 8x8 maxpolling\n",
        "    out = F.avg_pool2d(out, 8)\n",
        "    # view: pytorch에서 reshape과 같은 역할을 함\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjsulAB8VyOk"
      },
      "source": [
        "# ResNet20 함수 정의\n",
        "def ResNet20():\n",
        "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
        "  return ResNet(ResidualBlock, [3,3,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly81STdHWPj6"
      },
      "source": [
        "### CIFAR-10 DATASET LOADING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKnp-RRFaaQc"
      },
      "source": [
        "1. for training: 4 pixels are padded on each side,\n",
        "and a 32×32 crop is randomly sampled from the padded\n",
        "image or its horizontal flip\n",
        "2. For testing, we only evaluate\n",
        "the single view of the original 32×32 image\n",
        "\n",
        "3.  These models are trained with a minibatch size of 128 on two GPUs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuuY08eNYaax"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "                                      # 1번 조건\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                                     # 2번 조건: test dataset에 대해서는 augument 진행 X\n",
        "                                     transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./drive/MyDrive/Dev-dl/data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./drive/MyDrive/Dev-dl/data/', train=False, download=True, transform=transform_test)\n",
        "# Overfitting을 피하기 위해 shuffle=True로 지정해준다. (매 epoch마다 데이터셋 섞음)\n",
        "# two GPU사용\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6XtAc2ebGUp"
      },
      "source": [
        "### 환경설정 및 학습 함수 정의\n",
        "\n",
        "1. We use a weight decay of 0.0001 and momentum of 0.9,\n",
        "and adopt the weight initialization in [13] and BN [16] but\n",
        "with no dropout.\n",
        "\n",
        "2. We start with a learning\n",
        "rate of 0.1, divide it by 10 at 32k and 48k iterations, and\n",
        "terminate training at 64k iterations, which is determined on\n",
        "a 45k/5k train/val split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcPJ5Hb6bYIA",
        "collapsed": true
      },
      "source": [
        "device ='cuda'\n",
        "\n",
        "#신경망 선언\n",
        "net = ResNet20()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet20_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/ResNet20-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    # #100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/ResNet20-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "  torch.save(state, './checkpoint' + file_name)\n",
        "  print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcbK4Q2Sh52k"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWemAXcWlEWz",
        "collapsed": true
      },
      "source": [
        "import time\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
        "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
        "  if epoch >=90:\n",
        "    lr /= 10\n",
        "  if epoch >= 137:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0,150):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  print('\\n경과 시간:', time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeWGtKEInXAF"
      },
      "source": [
        "### ResNet-56 Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE1WkBzy3pHc"
      },
      "source": [
        "def ResNet56():\n",
        "  return ResNet(ResidualBlock, [9,9,9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HII5hVtY4Ezf",
        "collapsed": true
      },
      "source": [
        "#신경망 선언\n",
        "net = ResNet56()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet50_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/ResNet56-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    # 100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/ResNet56-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint' + file_name)\n",
        "    print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oVhc6NV8s4d",
        "collapsed": true
      },
      "source": [
        "import time\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
        "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
        "  if epoch >=90:\n",
        "    lr /= 10\n",
        "  if epoch >= 137:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0,150):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  print('\\n경과 시간:', time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY--I0Xj8uBa"
      },
      "source": [
        "#### Plain-20 Net Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMNFB4m32-Wu"
      },
      "source": [
        "# Plain Block class\n",
        "# Residual Block에서 shortcut connection 부분만 제외시키고 나머지는 동일하게 한다.\n",
        "class Block(nn.Module):\n",
        "  #in_plane : input의 dimension, planes: output의 dimension)\n",
        "  def __init__(self, in_planes, planes, stride=1):\n",
        "    #Block 호출 시 nn.Module 호출\n",
        "    super(Block, self).__init__()\n",
        "    # Convolution layer 정의\n",
        "    # Plain block은 두 개의 3x3 Conv layer로 구성되어 있으며 Conv => BN => Activation 과정을 거친다\n",
        "    # 논문에 따라 bias는 False로 지정\n",
        "    \n",
        "    # 첫 번째 Convolution Layer in Plain Block\n",
        "    # filter 수가 2배씩 증가하므로 너비 x 높이를 2배로 줄이고자 할때는 pooling이 아닌 stride를 2로 조정\n",
        "    self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "    # Conv layer를 거쳐 나온 output을 Batch Normalize\n",
        "    self.bn1 = nn.BatchNorm2d(planes)\n",
        "\n",
        "    # 두 번째 Convolution Layer in Plain Block\n",
        "    self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn2 = nn.BatchNorm2d(planes)\n",
        "    \n",
        "  #순전파 순서 정의\n",
        "  def forward(self, x):\n",
        "    # Conv => BN => Activation 순으로 진행\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.bn2(self.conv2(out))\n",
        "    out = F.relu(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXmM3mKI3i_l"
      },
      "source": [
        "# PlainNet 전체 Network 구성하는 Class\n",
        "class PlainNet(nn.Module):\n",
        "  #CIFAR-10 데이터셋의 클래스 10개에 맞추어 Parameter 조정\n",
        "  def __init__(self, block, num_blocks, num_classes=10):\n",
        "    super(PlainNet, self).__init__()\n",
        "    self.in_planes = 16\n",
        "\n",
        "    # 3의 input dimension(RGB)를 받아 16개 feature map 생성\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "    self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "    self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "    self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "  def _make_layer(self, block, planes, num_blocks, stride):\n",
        "    #num_blocks의 갯수 만큼 strides 리스트에 넣는다 => list의 length만큼 layer 내에 block 만든다\n",
        "    strides = [stride] + [1] * (num_blocks - 1)\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_planes, planes, stride))\n",
        "      self.in_planes = planes # 다음 layer로 넘어갈때 채널 수 맞춰주기\n",
        "    # *args: 가변 갯수의 인자를 함수에 집어넣어 줌\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  #순전파 방식\n",
        "  def forward(self, x):\n",
        "    out = F.relu(self.bn1(self.conv1(x)))\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out) #out: [batch_size, 64, 8,8]\n",
        "    #1x1로 바꿔주기 위해서 8x8 maxpolling\n",
        "    out = F.avg_pool2d(out, 8)\n",
        "    # view: pytorch에서 reshape과 같은 역할을 함\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0qrp2v14CNc"
      },
      "source": [
        "# Plain Net-20 함수 정의\n",
        "def PlainNet20():\n",
        "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
        "  return PlainNet(Block, [3,3,3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGLZeJuv4RAN",
        "collapsed": true
      },
      "source": [
        "#신경망 선언\n",
        "net = PlainNet20()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'plain20_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/Plain20-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    #100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/Plain20-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint' + file_name)\n",
        "    print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xLIgk9KS5O-l"
      },
      "source": [
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  lr = learning_rate\n",
        "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
        "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
        "  if epoch >=90:\n",
        "    lr /= 10\n",
        "  if epoch >= 137:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(0,150):\n",
        "  adjust_learning_rate(optimizer, epoch)\n",
        "  train(epoch)\n",
        "  test(epoch)\n",
        "  print('\\n경과 시간:', time.time()-start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lSdRq1RDl3i"
      },
      "source": [
        "### Plain-56 NET Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00oCUVLVDlfg"
      },
      "source": [
        "# PlainNet-56 함수 정의\n",
        "def PlainNet56():\n",
        "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
        "  return PlainNet(Block, [9,9,9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCZvFaXWDkzV"
      },
      "source": [
        "#신경망 선언\n",
        "net = PlainNet56()\n",
        "\n",
        "#신경망 GPU loading\n",
        "net = net.to(device) \n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'plain56_cifar.pth'\n",
        "\n",
        "# loss function => Cross-Entropy-Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
        "\n",
        "#학습 정의\n",
        "\n",
        "def train(epoch):\n",
        "  print('Epoch: %d'%epoch)\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    # loss back propagation\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "    writer.add_scalar(\"Loss/Plain56-train\", train_loss, epoch)\n",
        "    _, predicted = outputs.max(1)\n",
        "    #전체 갯수 count\n",
        "    total += targets.size(0)\n",
        "    #맞은 갯수 count\n",
        "    current_correct = predicted.eq(targets).sum().item()\n",
        "    correct += current_correct\n",
        "\n",
        "    #100 batch 마다 정확도 출력\n",
        "    # if batch_idx % 100 == 0:\n",
        "    #   print('\\nCurrent batch:', str(batch_idx))\n",
        "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
        "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
        "\n",
        "  print('\\nTotal average train accuracy:', correct / total)\n",
        "  print('Total average train loss:', train_loss / total)\n",
        "\n",
        "# 평가 정의\n",
        "\n",
        "def test(epoch):\n",
        "  print('\\n Test epoch: %d'%epoch)\n",
        "  net.eval()\n",
        "  loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += criterion(outputs, targets).item()\n",
        "        writer.add_scalar(\"Loss/Plain56-test\", loss, epoch)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "  print('\\nTotal average test accuarcy:', correct / total)\n",
        "  print('Total average test loss:', loss / total)\n",
        "\n",
        "  state = {\n",
        "        'net' : net.state_dict()\n",
        "    }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "    os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint' + file_name)\n",
        "    print('모델이 저장되었습니다')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}