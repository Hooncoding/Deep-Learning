{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757d61fc",
   "metadata": {},
   "source": [
    "# Identity Mappings in Deep Residual Networks 코드 실습\n",
    "\n",
    "## 목표\n",
    "1. shortcut connection은 identity Mapping이 최적임을 밝힘\n",
    "2. Foward 방식은 Full pre-activation이 최적임을 밝힘\n",
    "\n",
    "## 실험 방법\n",
    "\n",
    "### DataSet\n",
    "- CIFAR-10 Data Set\n",
    "\n",
    "### Model\n",
    "- ResNet-20\n",
    "    - Residual Block 구성에 따라 다른 model 구성\n",
    "        1. Original Block\n",
    "        2. ConstantScaledBlock (0.5 on Shortcut & Residual Function)\n",
    "        3. ExclusiveGating \n",
    "            - On Shortcut & Residual Function\n",
    "            - Only on Shortcut\n",
    "        4. ConvShortcout (1x1 conv on Shortcut)\n",
    "        5. DropoutShortcut (0.5 on Shortcut)\n",
    "    - Forward Process에 따라 다른 model 구성\n",
    "        1. BN After Addition\n",
    "        2. ReLu Before Addition\n",
    "        3. ReLu Only Pre-Activation\n",
    "        4. Full Pre-Activation\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1491cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= dataloader(train_dataset, batch_size = 128, shuffle=True, num_workers=1)\n",
    "test_loader = dataloader(test_dataset, batch_size = 100, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efabffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride):\n",
    "        super(OriginalBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.down_dim = out_channel - in_channel\n",
    "        self.pooling = nn.MaxPool2d(1, stride=stride)\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = F.relu(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.stride != 1:\n",
    "            out += self.pooling(F.pad(x, (0, 0, 0, 0, 0 ,self.down_dim)))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c605c1c1",
   "metadata": {},
   "source": [
    "### Shortcut 형태에 따라 다른 Block 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5e9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantScaledBlock(OriginalBlock):\n",
    "    def __init__(self, in_channel, out_channel, stride):\n",
    "        super().__init__(in_channel, out_channel, stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = F.relu(out)\n",
    "        out = torch.mul(self.bn2(self.conv2(out)), 0.5) #Residual Function Constant Scaled\n",
    "        if self.stride != 1:\n",
    "            out = out + torch.mul(self.pooling(F.pad(x, (0, 0, 0, 0, 0, self.down_dim))),0.5) # shortcut Constant Scaled\n",
    "        else:\n",
    "            out = out + torch.mul(x,0.5) # shortcut Constant Scaled\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExclusiveGating(OriginalBlock):\n",
    "    def __init__(self, in_channel, out_channel, stride):\n",
    "        super().__init__(in_channel, out_channel, stride)\n",
    "        self.gating = nn.Conv2d(out_channel, out_channel, kernel_size=1, stride=1, bias= -6) #g(x) = wx + b 로 만들어주기\n",
    "        self.gating_down = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride, bias=-6) # in_dim != out_dim일 시 featuremap size downsample\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = F.relu(out)\n",
    "        out = F.sigmoid(self.gating(self.bn2(self.conv2(out)))) #g(x)\n",
    "        if self.stride != 1:\n",
    "            out = out + (1 - F.sigmoid(self.gating_down(x))) # on shortcut: 1-g(x)\n",
    "        else:\n",
    "            out = out + (1 - F.sigmoid(self.gating(x)))\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcea50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortCutOnlyGating(ExclusiveGating):\n",
    "  def __init__(self, in_channel, out_channel, stride):\n",
    "    super().__init__(in_channel, out_channel, stride)\n",
    "  def forward(self, x):\n",
    "    out = self.bn1(self.conv1(x))\n",
    "    out = F.relu(out)\n",
    "    out = self.bn2(self.conv2(out)) # gating only in shortcut\n",
    "    if self.stride != 1:\n",
    "      out = out + (1 - F.sigmoid(self.gating_down(x)))\n",
    "    else:\n",
    "      out = out + (1 - F.sigmoid(self.gating(x)))\n",
    "    out = F.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvShortcut(OriginalBlock):\n",
    "  def __init__(self, in_channel, out_channel, stride):\n",
    "    super().__init__(in_channel, out_channel, stride)\n",
    "    self.conv_shortcut = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1) # 1x1 conv when in_dim == out_dim \n",
    "    self.conv_shortcut_down = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride) # 1x1 conv when in_dim != out_dim\n",
    "  def forward(self, x):\n",
    "    out = self.bn1(self.conv1(x))\n",
    "    out = F.relu(out)\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    if self.stride != 1:\n",
    "      out = out + self.conv_shortcut_down(x)\n",
    "    else:\n",
    "      out = out + self.conv_shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864221ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutShortcut(OriginalBlock):\n",
    "  def __init__(self, in_channel, out_channel, stride):\n",
    "    super().__init__(in_channel, out_channel, stride)\n",
    "  def forward(self, x):\n",
    "    out = self.bn1(self.conv1(x))\n",
    "    out = F.relu(out)\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    if self.stride != 1:\n",
    "      out = out + F.dropout(self.pooling(F.pad(x, (0, 0, 0, 0, 0 ,self.down_dim)))) # Dropout in shortcut\n",
    "    else:\n",
    "      out = out + F.dropout(x)\n",
    "    out = F.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d05b17",
   "metadata": {},
   "source": [
    "### Forward 순서 변경에 따른 Block 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalBlock_BNAfterAddition(OriginalBlock):\n",
    "    # Conv => BN => ReLu => Conv => Shortcut Addition => BN => ReLu\n",
    "  def forward(self,x):\n",
    "    out = self.bn1(self.conv1(x))\n",
    "    out = F.relu(out)\n",
    "    out = self.conv2(out)\n",
    "    if self.stride != 1:\n",
    "      out = out + self.pooling(F.pad(x, (0, 0, 0, 0, 0, self.down_dim)))\n",
    "    else:\n",
    "      out = out + x\n",
    "    out = F.relu(self.bn2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41de990",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalBlock_ReLUBeforeAddition(OriginalBlock):\n",
    "    # Conv => BN => Relu => Conv => BN => Relu => Shortcut Addition\n",
    "  def forward(self,x):\n",
    "    out = self.bn1(self.conv1(x))\n",
    "    out = F.relu(out)\n",
    "    out = F.relu(self.bn2(self.conv2(out)))\n",
    "    if self.stride != 1:\n",
    "      out = out + self.pooling(F.pad(x, (0, 0, 0, 0, 0, self.down_dim)))\n",
    "    else:\n",
    "      out = out + x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ec3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalBlock_ReLUOnlyPreActivation(OriginalBlock):\n",
    "    # ReLu => ConV => BN => Relu => Conv => BN => Shortcut Addition\n",
    "  def forward(self, x):\n",
    "    out = F.relu(x)\n",
    "    out = self.bn1(self.conv1(out))\n",
    "    out = F.relu(out)\n",
    "    out = self.bn2(self.conv2(out))\n",
    "    if self.stride != 1:\n",
    "      out = out + self.pooling(F.pad(x, (0, 0, 0, 0, 0, self.down_dim)))\n",
    "    else:\n",
    "      out = out + x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OriginalBlock_FullPreActivation(OriginalBlock):\n",
    "    # BN => ReLu => Conv => BN => ReLu => Conv2 => Shortcut Addition\n",
    "  def __init__(self, in_channel, out_channel, stride):\n",
    "    super().__init__(in_channel, out_channel, stride)\n",
    "    self.bn1 = nn.BatchNorm2d(in_channel)\n",
    "  def forward(self, x):\n",
    "    out = self.bn1(x)\n",
    "    out = F.relu(out)\n",
    "    out = self.conv1(out)\n",
    "    out = self.bn2(out)\n",
    "    out = F.relu(out)\n",
    "    out = self.conv2(out)\n",
    "    if self.stride != 1:\n",
    "      out = out + self.pooling(F.pad(x,(0,0,0,0,0, self.down_dim)))\n",
    "    else:\n",
    "      out = out + x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ac714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 전체 Network 구성하는 Class\n",
    "class ResNet(nn.Module):\n",
    "  #CIFAR-10 데이터셋의 클래스 10개에 맞추어 Parameter 조정\n",
    "  def __init__(self, block, num_blocks, num_classes=10):\n",
    "    super(ResNet, self).__init__()\n",
    "    self.in_planes = 16\n",
    "\n",
    "    # ImageNet 처리 시 가장 앞 단에 layer 7x7, maxPooling 층을 두었으나\n",
    "    # 3의 input dimension(RGB)를 받아 64개 feature map 생성\n",
    "    self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    self.bn1 = nn.BatchNorm2d(16)\n",
    "    self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "    self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "    self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "    self.linear = nn.Linear(64, num_classes)\n",
    "    self.softmax = nn.Softmax(1)\n",
    "\n",
    "  def _make_layer(self, block, planes, num_blocks, stride):\n",
    "    #num_blocks의 갯수 만큼 strides 리스트에 넣는다 => list의 length만큼 layer 내에 block 만든다\n",
    "    strides = [stride] + [1] * (num_blocks - 1)\n",
    "    layers = []\n",
    "    for stride in strides:\n",
    "      layers.append(block(self.in_planes, planes, stride))\n",
    "      self.in_planes = planes # 다음 layer로 넘어갈때 채널 수 맞춰주기\n",
    "    # *args: 가변 갯수의 인자를 함수에 집어넣어 줌\n",
    "    return nn.Sequential(*layers)\n",
    "  \n",
    "  #순전파 방식\n",
    "  def forward(self, x):\n",
    "    out = F.relu(self.bn1(self.conv1(x)))\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out) #out: [batch_size, 64, 8,8]\n",
    "    #1x1로 바꿔주기 위해서 8x8 maxpolling\n",
    "    out = F.avg_pool2d(out, 8)\n",
    "    # view: pytorch에서 reshape과 같은 역할을 함\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.linear(out)\n",
    "#     out = self.softmax(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0bb01",
   "metadata": {},
   "source": [
    "#### ResNet20 with Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet20 함수 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(OriginalBlock, [3,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e678071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library Import for Visualization\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./runs/OriginalBlock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d3bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda'\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet110_original.pth'\n",
    "\n",
    "# loss function => Cross-Entropy-Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "#학습 정의\n",
    "\n",
    "def train(epoch):\n",
    "  print('Epoch: %d'%epoch)\n",
    "  net.train()\n",
    "  train_loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  error = 0\n",
    "\n",
    "  for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    # loss back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    writer.add_scalar(\"Train Loss\", train_loss, epoch)\n",
    "    _, predicted = outputs.max(1)\n",
    "    #전체 갯수 count\n",
    "    total += targets.size(0)\n",
    "    #맞은 갯수 count\n",
    "    current_correct = predicted.eq(targets).sum().item()\n",
    "    correct += current_correct\n",
    "    error = ((total - correct) / total) * 100\n",
    "    writer.add_scalar(\"Train Error\", error, epoch)\n",
    "\n",
    "    # #100 batch 마다 정확도 출력\n",
    "    # if batch_idx % 100 == 0:\n",
    "    #   print('\\nCurrent batch:', str(batch_idx))\n",
    "    #   print('Current batch average train accuracy:', current_correct / targets.size(0))\n",
    "    #   print('Current batch average train loss:', loss.item() / targets.size(0))\n",
    "\n",
    "  print('\\nTotal average train accuracy:', correct / total)\n",
    "  print('Total average train loss:', train_loss / total)\n",
    "\n",
    "# 평가 정의\n",
    "\n",
    "def test(epoch):\n",
    "  print('\\n Test epoch: %d'%epoch)\n",
    "  net.eval()\n",
    "  loss = 0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  error = 0\n",
    "\n",
    "  for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        total += targets.size(0)\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss += criterion(outputs, targets).item()\n",
    "        writer.add_scalar(\"Test Loss\", loss, epoch)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        error = ((total - correct) / total) * 100\n",
    "        writer.add_scalar(\"Test Error\", error, epoch)\n",
    "        \n",
    "  print('\\nTotal average test accuarcy:', correct / total)\n",
    "  print('Total average test loss:', loss / total)\n",
    "\n",
    "  state = {\n",
    "        'net' : net.state_dict()\n",
    "    }\n",
    "  if not os.path.isdir('checkpoint'):\n",
    "    os.mkdir('checkpoint')\n",
    "  torch.save(state, './checkpoint' + file_name)\n",
    "  print('모델이 저장되었습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06dd2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "  lr = learning_rate\n",
    "  # iteration in 1 epoch = train_data size / batch size = 45000/128 = 약 350\n",
    "  # 32000, 48000에서 lr update => 32000/350 = 약 90번째 epoch, 48000/350 = 137번째 epoch\n",
    "  if epoch >=90:\n",
    "    lr /= 10\n",
    "  if epoch >= 137:\n",
    "    lr /= 10\n",
    "  for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = lr\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b56a5c",
   "metadata": {},
   "source": [
    "#### ResNet20 with ConstantScaledBlock (0.5 on Shortcut & Residual Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbda315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(ConstantScaledBlock, [3,3,3])\n",
    "writer = SummaryWriter('./runs/ConstantScaledBlock')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet110_ConstantScaled.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21908d66",
   "metadata": {},
   "source": [
    "#### ResNet20 with ExclusiveGating (On Shortcut & Residual Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487df86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(ExclusiveGating, [3,3,3])\n",
    "writer = SummaryWriter('./runs/ExclusiveGatingBlock')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet110_ExclusiveGating.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7d1fb",
   "metadata": {},
   "source": [
    "#### ResNet20 with ExclusiveGating (Only Shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be162c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(ShortCutOnlyGating, [3,3,3])\n",
    "writer = SummaryWriter('./runs/ShortCutGatingBlock')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet110_ShortCutGating.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8666d048",
   "metadata": {},
   "source": [
    "#### ResNet20 with ConvShortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(ConvShortcut, [3,3,3])\n",
    "writer = SummaryWriter('./runs/ConvShortcutBlock')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet110_ConvShortcut.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b6f5a0",
   "metadata": {},
   "source": [
    "#### ResNet20 with DropoutShortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63adb36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(DropoutShortcut, [3,3,3])\n",
    "writer = SummaryWriter('./runs/DropoutShortcutBlock')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet110_DropoutShortcut.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d28ab",
   "metadata": {},
   "source": [
    "### Forward 순서에 따른 모델 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a7e6ba",
   "metadata": {},
   "source": [
    "- Foward step에 따라 다른 model 구성\n",
    "        1. BN After Addition\n",
    "        2. ReLu Before Addition\n",
    "        3. ReLu Only Pre-Activation\n",
    "        4. Full Pre-Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d9182",
   "metadata": {},
   "source": [
    "#### ResNet20 with BN After Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdff79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(OriginalBlock_BNAfterAddition, [3,3,3])\n",
    "writer = SummaryWriter('./runs/BN_after_Addition')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet20_BNafterAddition.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f44e7e",
   "metadata": {},
   "source": [
    "#### ResNet20 with ReLu Before Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65a2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(OriginalBlock_ReLUBeforeAddition, [3,3,3])\n",
    "writer = SummaryWriter('./runs/ReLu_Before_Addition')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet20_RelubeforeAddition.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf31c8",
   "metadata": {},
   "source": [
    "#### ResNet20 with ReLu Only Pre-Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181184e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(OriginalBlock_ReLUOnlyPreActivation, [3,3,3])\n",
    "writer = SummaryWriter('./runs/ReLu_Only_PreActivation')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet20_ReluonlyPreActivation.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1d3ae",
   "metadata": {},
   "source": [
    "#### ResNet20 with Full Pre-Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e40344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(OriginalBlock_FullPreActivation, [3,3,3])\n",
    "writer = SummaryWriter('./runs/Full_PreActivation')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet20_Fullpreactivation.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec73dd",
   "metadata": {},
   "source": [
    "#### ResNet20 with Conv Shortcut & Full Pre-Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b530ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvShortcut_FullPA_Block(OriginalBlock):\n",
    "  def __init__(self, in_channel, out_channel, stride):\n",
    "    super().__init__(in_channel, out_channel, stride)\n",
    "    self.bn1 = nn.BatchNorm2d(in_channel)\n",
    "    self.conv_shortcut = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=1) # 1x1 conv when in_dim == out_dim \n",
    "    self.conv_shortcut_down = nn.Conv2d(in_channel, out_channel, kernel_size=1, stride=stride) # 1x1 conv when in_dim != out_dim\n",
    "  def forward(self, x):\n",
    "    out = self.bn1(x)\n",
    "    out = F.relu(out)\n",
    "    out = self.conv1(out)\n",
    "    out = self.bn2(out)\n",
    "    out = F.relu(out)\n",
    "    out = self.conv2(out)\n",
    "    if self.stride != 1:\n",
    "      out = out + self.conv_shortcut_down(x)\n",
    "    else:\n",
    "      out = out + self.conv_shortcut(x)\n",
    "    out = F.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4acec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net 정의\n",
    "def ResNet20():\n",
    "  # 2개의 convolution layer 으로 구성된 블록이 layer마다 3개 있으므로 전체 레이어는 6n개\n",
    "  return ResNet(ConvShortcut_FullPA_Block, [3,3,3])\n",
    "writer = SummaryWriter('./runs/Conv&Full_PreActivation')\n",
    "\n",
    "#신경망 선언\n",
    "net = ResNet20()\n",
    "\n",
    "#신경망 GPU loading\n",
    "net = net.to(device) \n",
    "\n",
    "learning_rate = 0.1\n",
    "file_name = 'resnet20_Fullpreactivation.pth'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(0,150):\n",
    "  adjust_learning_rate(optimizer, epoch)\n",
    "  train(epoch)\n",
    "  test(epoch)\n",
    "  print('\\n경과 시간:', time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-dl",
   "language": "python",
   "name": "dev-dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
